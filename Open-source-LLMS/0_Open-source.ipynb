{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Source LLMs\n",
    "* No need cost\n",
    "* Free to use for research purposes and commercial purposes\n",
    "* They are not hosted anywhere and will be available in hugging face.\n",
    "* You have to download that model and load that that model\n",
    "* One of the disadvantage is that you don't have any api access.You need good CPU configuration system\n",
    "   - Core i3 - i5 processor\n",
    "   - 8 gb ram\n",
    "   - GPU will be a plus point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some very popular and powerful Open source LLM models\n",
    "\n",
    "* Meta LLama 3\n",
    "* Meta LLama 2\n",
    "* Google Palm 2\n",
    "* Falcon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "Quantization is a technique to reduce the computational and memory costs of running inference by representing the weights and activations with low-precision data types like 8-bit integer (int8) instead of the usual 32-bit floating point (float32).\n",
    "\n",
    "Reducing the number of bits means the resulting model requires less memory storage, consumes less energy (in theory), and operations like matrix multiplication can be performed much faster with integer arithmetic. It also allows to run models on embedded devices, which sometimes only support integer data types.\n",
    "\n",
    "For example Converting 3.2 into 3 means from float to integer so it will have lesser memory.If your model size is  30 gb then after quantization it will become 5 gb so you will use it in low configuration pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
