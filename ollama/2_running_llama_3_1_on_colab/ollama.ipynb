{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "```sh\n",
    "# Step 1: Install Terminal, load and launch terminal\n",
    "pip install colab-xterm\n",
    "\n",
    "# Step 2: Launching Terminal\n",
    "# Load terminal extension\n",
    "%load_ext colabxterm\n",
    "# Launch terminal\n",
    "%xterm\n",
    "\n",
    "# Step 3: Install Ollama (Make sure to run both of these commands inside runtime Terminal)\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# Step 4: Download and Run LLM Model\n",
    "ollama serve & ollama run llama3.1\n",
    "```\n",
    "\n",
    "# Step 5: Test Ollama Model\n",
    "ollama_url = \"http://127.0.0.1:11434\"\n",
    "\n",
    "def query_ollama(prompt, model=\"llama3.1\"):\n",
    "    data = {\n",
    "        \"prompt\": prompt,\n",
    "        \"model\": model,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    response = requests.post(f\"{ollama_url}/api/generate\", json=data)\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get(\"response\", \"No response Found\")\n",
    "    else:\n",
    "        return f\"Error: {response.status_code}, {response.text}\"\n",
    "\n",
    "response = query_ollama(\"Greet me in 3 words!\")\n",
    "print(response)\n",
    "\n",
    "prompt = \"Write me a poem about 5 sentence about Prophet Muhmmad SAW\"\n",
    "result = query_ollama(prompt)\n",
    "print(result)\n",
    "\n",
    "prompt = \"give me fastapi hello world code\"\n",
    "result = query_ollama(prompt)\n",
    "print(result)\n",
    "\n",
    "# Streaming API Response\n",
    "def query_ollama_streaming(prompt, model=\"llama3.1\"):\n",
    "    headers = {\n",
    "        \"ngrok-skip-browser-warning\": \"true\"  # This header bypasses the Ngrok browser warning\n",
    "    }\n",
    "    data = {\n",
    "        \"prompt\": prompt,\n",
    "        \"model\": model,\n",
    "        \"stream\": True\n",
    "    }\n",
    "\n",
    "    # Stream the request to handle the sequence of JSON objects\n",
    "    response = requests.post(f\"{ollama_url}/api/generate\", json=data, headers=headers, stream=True)\n",
    "\n",
    "    # Extract the text response\n",
    "    text_response = \"\"\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            try:\n",
    "                json_data = json.loads(line)\n",
    "                if \"response\" in json_data:\n",
    "                    text_response += json_data[\"response\"]\n",
    "                    print(f\"Partial response: {text_response}\")\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "    return text_response\n",
    "\n",
    "# Test the connection\n",
    "response_text = query_ollama(\"What is Generative AI?\")\n",
    "print(response_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
