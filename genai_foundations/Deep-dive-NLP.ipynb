{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early Approaches in NLP: Counting Words\n",
    "Before neural networks became popular, NLP (Natural Language Processing) relied on simpler techniques that focused on counting words. Two main methods were:\n",
    "\n",
    "## Count Vectors\n",
    "- Imagine you have a document, and you simply count how often each word appears.\n",
    "- **Example**: In the sentence \"I like cats and I like dogs\", the word \"like\" appears 2 times, \"I\" appears 2 times, and the rest appear once.\n",
    "\n",
    "## TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "- It’s a bit more advanced than just counting.\n",
    "- TF-IDF assigns importance to words:\n",
    "  - Words that appear often in a document are important for that document.\n",
    "  - But if a word is too common across many documents (like \"the\" or \"is\"), it becomes less important.\n",
    "- **Example**: If the word \"panda\" appears frequently in one document but rarely in others, TF-IDF will assign it high importance for that specific document.\n",
    "\n",
    "These methods were good at categorizing and searching through text, but they had a problem:\n",
    "- They couldn’t understand the meaning or relationships between words. For example, \"happy\" and \"joyful\" are similar in meaning, but these methods treated them as completely different words.\n",
    "\n",
    "# Neural Networks Enter the Scene\n",
    "To solve these limitations, neural networks (NNs) started being used for NLP. A big breakthrough came in 2003 from a researcher named Yoshua Bengio and his team, who developed the Neural Network Language Model (NNLM).\n",
    "\n",
    "## How NNLM Worked\n",
    "- It predicted the next word in a sentence by looking at the previous words.\n",
    "  - **Example**: If the input is \"I like\", the NNLM would try to predict that the next word could be \"cats\" or \"dogs\".\n",
    "\n",
    "## Word Embeddings\n",
    "- The NNLM didn’t just treat words as isolated counts. It created **word embeddings** — compact vectors (numbers) that captured the meaning of words.\n",
    "  - **Example**: Words like \"happy\" and \"joyful\" would have similar word embeddings, showing they are related in meaning.\n",
    "\n",
    "## Limitations of the NNLM\n",
    "- It struggled with long sentences and large vocabularies (lots of words).\n",
    "- It wasn’t perfect, but it was a major improvement over counting words because it could understand meaning better.\n",
    "\n",
    "# Summary: From Counting to Understanding\n",
    "- **Old methods** (Count Vectors, TF-IDF) were good at counting words but couldn’t understand meaning or context.\n",
    "- **Neural Networks** (like NNLM) introduced word embeddings that helped capture semantic relationships (like \"happy\" and \"joyful\" being similar).\n",
    "- Though the NNLM had some problems with long texts and large vocabularies, it opened the door to better neural models in the future.\n",
    "\n",
    "# What Are Word Embeddings?\n",
    "Think of word embeddings as a way to represent words using numbers so that a computer can understand their meaning and relationships.\n",
    "\n",
    "## Imagine a World Without Embeddings\n",
    "- Before embeddings, computers saw words as just plain text with no connection to each other.\n",
    "- **Example**:\n",
    "  - \"dog\" and \"cat\" were treated like completely different words, even though they both refer to animals.\n",
    "  - Similarly, \"happy\" and \"joyful\" had no relation, even though they mean almost the same thing.\n",
    "\n",
    "## Enter Word Embeddings\n",
    "- A word embedding is a set of numbers (a vector) that represents a word in a way that captures its meaning and relationships with other words.\n",
    "  - **Example**:\n",
    "    - \"dog\" might be represented as [0.5, 0.8, -0.2].\n",
    "    - \"cat\" might be [0.6, 0.7, -0.1].\n",
    "    - These two vectors are close to each other, meaning the computer understands that \"dog\" and \"cat\" are related.\n",
    "\n",
    "## How Word Embeddings Work in Practice\n",
    "- The magic happens because the closer two word embeddings are in the number space, the more similar the words are in meaning.\n",
    "  - **Example**:\n",
    "    - \"happy\" might have the vector [0.3, 0.9, -0.4].\n",
    "    - \"joyful\" might have [0.3, 0.8, -0.3].\n",
    "    - These vectors are close, meaning the computer knows they are similar in meaning.\n",
    "\n",
    "## Analogy: Coordinates on a Map\n",
    "- Think of word embeddings as placing words on a map:\n",
    "  - Words like \"king\" and \"queen\" will be close to each other.\n",
    "  - Words like \"apple\" and \"banana\" will also be close to each other, but far from \"king\" and \"queen\" because they belong to different categories.\n",
    "\n",
    "# Distributed Representations: Making Word Meanings Understandable for Computers\n",
    "In natural language processing (NLP), distributed representations are a smart way to represent words so computers can understand their meaning and relationships.\n",
    "- Instead of treating words as just individual symbols, we use **vectors** (sets of numbers) to describe them, and each number in the vector captures some feature of the word.\n",
    "\n",
    "# Two Key Techniques for Creating Word Vectors\n",
    "## Word2Vec\n",
    "- **How it works**: Word2Vec uses neural networks to predict words that appear around each word in a text.\n",
    "  - **Example**: If your sentence is “The cat is sleeping,” Word2Vec will try to predict the surrounding words for “cat”.\n",
    "  - This process helps the neural network figure out how often words like “cat” and “sleeping” appear together.\n",
    "- **Result**: Each word gets a vector (a set of numbers) that reflects how it is used in context. Words with similar meanings will have vectors that are close together.\n",
    "\n",
    "## GloVe (Global Vectors)\n",
    "- **How it works**: GloVe takes a different approach by analyzing how often words appear together in a large collection of text (called co-occurrence).\n",
    "  - **Example**: If it notices that “coffee” often appears with “hot”, and “ice cream” with “cold”, it will create vectors that reflect these relationships.\n",
    "    - Coffee: [1, 0]\n",
    "    - Hot: [0.9, 0]\n",
    "    - Ice Cream: [0, 1]\n",
    "    - Cold: [0, 0.9]\n",
    "  - **Result**: The vectors for “coffee” and “hot” will be close together, and similarly for “ice cream” and “cold.” This reflects the fact that they are related concepts.\n",
    "\n",
    "# What is Transfer Learning in NLP?\n",
    "Transfer learning allows a model to re-use knowledge it gained from one task and apply it to another, saving time and effort.\n",
    "\n",
    "- **Example**:\n",
    "  - Let’s say we train a model to understand sentiments (positive or negative) in movie reviews.\n",
    "  - It learns that words like \"amazing\" are positive and \"terrible\" are negative.\n",
    "  - Now, we want to apply this to product reviews. Instead of training a new model from scratch, we re-use the existing knowledge.\n",
    "  - The model can now understand product-related sentiments faster, even with less data.\n",
    "\n",
    "# How Transfer Learning Works in NLP\n",
    "- **Pre-trained Models**:\n",
    "  - Models like GloVe create word vectors by analyzing huge collections of text. These vectors capture the meaning and relationships between words.\n",
    "  - This base knowledge can be re-used for other tasks without starting from scratch.\n",
    "\n",
    "# How NNs Improved NLP with Transfer Learning\n",
    "- **Automatic Learning of Patterns**:\n",
    "  - Neural networks don’t need hand-made rules. They can learn complex patterns (like context and relationships) directly from data.\n",
    "- **Richer Representations with NNs**:\n",
    "  - NNs produce even better word vectors than GloVe or Word2Vec, capturing deeper meanings.\n",
    "- **Examples of NN Architectures**:\n",
    "  - CNNs (Convolutional Neural Networks): Great at tasks like text classification.\n",
    "  - RNNs (Recurrent Neural Networks): Useful for sequences, like predicting the next word in a sentence.\n",
    "  - Transformers (like BERT and GPT): A game-changer in NLP because they can understand context in a sentence better than earlier models.\n",
    "\n",
    "# Summary\n",
    "- **Old Methods** (Count Vectors, TF-IDF) were good at counting words but couldn’t understand meaning.\n",
    "- **Word Embeddings** (Word2Vec, GloVe) made it possible for computers to understand relationships between words.\n",
    "- **Neural Networks & Transfer Learning** brought richer ways to understand language, allowing models to apply knowledge across different tasks.\n",
    "\n",
    "\n",
    "# Language Modeling with RNNs: Making Sense of Sequences\n",
    "\n",
    "Think of RNNs (Recurrent Neural Networks) as a way for computers to understand and process sequences like text. Unlike simple neural networks, RNNs remember what happened earlier in a sentence to make sense of what comes next.\n",
    "\n",
    "## How RNNs Work\n",
    "\n",
    "### Sequence Processing:\n",
    "- RNNs read text one word at a time, passing information from one step to the next. This way, each word in a sentence influences the understanding of the next word.\n",
    "- **Example:** In the phrase \"I love\", RNNs can predict the next word might be \"chocolate\" or \"music\" based on what it has already read.\n",
    "\n",
    "### Internal Memory:\n",
    "- RNNs have an internal state that helps them remember what came earlier in the sequence, like how we remember the subject of a sentence to make sense of its ending.\n",
    "\n",
    "## The Problem with RNNs\n",
    "\n",
    "RNNs have trouble remembering information over long sequences. For example:\n",
    "- In a long sentence like \"The cat, which already ate a lot, was not hungry,\" standard RNNs might forget \"The cat\" by the time they reach \"was not hungry\".\n",
    "- This issue is known as the **vanishing gradient problem**, where earlier information gets lost as the sequence gets longer.\n",
    "\n",
    "## LSTMs: A Better RNN for Long Sequences\n",
    "\n",
    "To solve this, LSTMs (Long Short-Term Memory Networks) were introduced.\n",
    "\n",
    "### How LSTMs Work:\n",
    "- LSTMs use a system of **gates** to decide:\n",
    "  - What information to keep.\n",
    "  - What information to forget.\n",
    "  - What new information to add.\n",
    "- These gates help LSTMs maintain important information over long sequences.\n",
    "\n",
    "### Example of LSTM in Action:\n",
    "- In the sentence \"The cat, which already ate a lot, was not hungry,\" the LSTM remembers:\n",
    "  - The subject \"The cat\" from the start.\n",
    "  - The recent information that the cat ate a lot, helping it predict the next word \"hungry\".\n",
    "- LSTMs excel at both short-term and long-term memory, making them great for language modeling and text classification tasks.\n",
    "\n",
    "## Rise of CNNs in NLP\n",
    "\n",
    "While RNNs are good at understanding sequences, they can be slow because they process text one step at a time. **CNNs (Convolutional Neural Networks)**, originally designed for images, offer a faster alternative for NLP.\n",
    "\n",
    "### How CNNs Work in NLP:\n",
    "- CNNs use **filters** to scan through text, identifying patterns in n-grams (short word sequences).\n",
    "- **Example:** A CNN can identify phrases like \"great service\" or \"terrible experience\".\n",
    "\n",
    "### Feature Extraction in CNNs:\n",
    "- **Lower Layers:** Detect simple word patterns like common phrases.\n",
    "- **Middle Layers:** Understand negations (e.g., \"not good\").\n",
    "- **Higher Layers:** Capture abstract concepts like the overall sentiment of a review.\n",
    "\n",
    "### Advantages of CNNs:\n",
    "- **Parallel Processing:** CNNs process text in parallel (faster than RNNs, which process word by word).\n",
    "- **Hierarchical Features:** They extract features at different levels, improving the understanding of text.\n",
    "\n",
    "## Limitations of CNNs and the Rise of Self-Attention\n",
    "\n",
    "While CNNs are fast, they can only capture **local information** (nearby words) and miss global relationships in long sequences. This led to the development of **attention-augmented networks**, which combine CNNs with self-attention to capture relationships across the entire input."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
