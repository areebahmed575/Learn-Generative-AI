{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U langgraph langgraph-checkpoint-postgres psycopg psycopg-pool langchain_google_genai\n"
      ],
      "metadata": {
        "id": "e1fCWxH3XofU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Gemini API key\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"home-services-demo\""
      ],
      "metadata": {
        "id": "I0gtyoQxXtIN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DB_URI = userdata.get('DB_URI')  # Example: \"postgresql://user:pass@host:5432/dbname\"\n",
        "\n",
        "from psycopg_pool import ConnectionPool\n",
        "from langgraph.checkpoint.postgres import PostgresSaver\n",
        "\n",
        "connection_kwargs = {\"autocommit\": True, \"prepare_threshold\": 0}\n",
        "pool = ConnectionPool(conninfo=DB_URI, max_size=20, kwargs=connection_kwargs)\n",
        "checkpointer = PostgresSaver(pool)\n",
        "checkpointer.setup()  # create required tables"
      ],
      "metadata": {
        "id": "sBW2mLN1Xv0Y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.types import interrupt\n",
        "from langgraph.graph.state import CompiledStateGraph\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "\n",
        "# State schema\n",
        "class ServiceState(dict):\n",
        "    messages: list\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", api_key=GEMINI_API_KEY)"
      ],
      "metadata": {
        "id": "01nR4poJXy08"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def intake(state: ServiceState) -> ServiceState:\n",
        "    print(\"Intake step running...\")\n",
        "    return state\n",
        "\n",
        "# Diagnosis step\n",
        "def diagnosis(state: ServiceState) -> ServiceState:\n",
        "    user_message = state[\"messages\"][-1].content.lower()\n",
        "\n",
        "    # If expensive issue -> raise interrupt\n",
        "    if \"replace heater\" in user_message or \"expensive\" in user_message:\n",
        "        raise interrupt(\"⚠️ Expensive repair detected. Needs technician approval.\")\n",
        "\n",
        "    # Otherwise let model respond safely\n",
        "    response = llm.invoke(state[\"messages\"])\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "# Technician approval step\n",
        "def technician_review(state: ServiceState) -> ServiceState:\n",
        "    # This is a placeholder node\n",
        "    # Technician will inject updates using graph.update_state()\n",
        "    return state"
      ],
      "metadata": {
        "id": "fmmh7M3hX1ye"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "builder = StateGraph(ServiceState)\n",
        "\n",
        "builder.add_node(\"intake\", intake)\n",
        "builder.add_node(\"diagnosis\", diagnosis)\n",
        "builder.add_node(\"technician_review\", technician_review)\n",
        "\n",
        "builder.add_edge(START, \"intake\")\n",
        "builder.add_edge(\"intake\", \"diagnosis\")\n",
        "builder.add_edge(\"diagnosis\", END)\n",
        "\n",
        "graph: CompiledStateGraph = builder.compile(checkpointer=checkpointer)\n"
      ],
      "metadata": {
        "id": "RAJITHDqX33i"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thread_config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "customer_msg = HumanMessage(content=\"No hot water, should I replace heater? (expensive)\")\n",
        "print(\"=== Customer starts ===\")\n",
        "for event in graph.stream({\"messages\": [customer_msg]}, thread_config, stream_mode=\"values\"):\n",
        "    print(event)\n",
        "\n",
        "# Check paused state\n",
        "state = graph.get_state(thread_config)\n",
        "print(\"\\n--- Graph paused state ---\")\n",
        "print(\"Next:\", state.next)\n",
        "print(\"Interrupts:\", state.tasks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2Hq0L72X54c",
        "outputId": "1436372c-e18d-4b17-9141-d66adf32f21c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Customer starts ===\n",
            "{'messages': [HumanMessage(content='No hot water, should I replace heater? (expensive)', additional_kwargs={}, response_metadata={})]}\n",
            "Intake step running...\n",
            "{'messages': [HumanMessage(content='No hot water, should I replace heater? (expensive)', additional_kwargs={}, response_metadata={})]}\n",
            "\n",
            "--- Graph paused state ---\n",
            "Next: ('diagnosis',)\n",
            "Interrupts: (PregelTask(id='f4c2c534-9bc6-e5f0-c96e-57ad2e5bdf60', name='diagnosis', path=('__pregel_pull', 'diagnosis'), error=None, interrupts=(Interrupt(value='⚠️ Expensive repair detected. Needs technician approval.', id='dd51336e3ed2fad96d650358888729bf'),), state=None, result=None),)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Technician logs in ===\")\n",
        "for m in state.values['messages']:\n",
        "    print(type(m).__name__, \":\", m.content)\n",
        "\n",
        "# Technician edits the recommendation\n",
        "from langchain_core.messages import HumanMessage\n",
        "graph.update_state(\n",
        "    thread_config,\n",
        "    {\"messages\": [HumanMessage(content=\"Technician: Suggest flushing tank first before replacement.\")]}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5PnUlD3X75z",
        "outputId": "1477d6fa-289c-4b11-fe62-6ea0023671c2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Technician logs in ===\n",
            "HumanMessage : No hot water, should I replace heater? (expensive)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'configurable': {'thread_id': '1',\n",
              "  'checkpoint_ns': '',\n",
              "  'checkpoint_id': '1f098727-015e-6fd3-8002-06746c39a346'}}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Resuming after Technician approval ===\")\n",
        "for event in graph.stream(None, thread_config, stream_mode=\"values\"):\n",
        "    print(event)\n",
        "\n",
        "# Final state check\n",
        "final_state = graph.get_state(thread_config)\n",
        "print(\"\\n--- Final State ---\")\n",
        "for m in final_state.values['messages']:\n",
        "    print(type(m).__name__, \":\", m.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zg2L8HHNYAA7",
        "outputId": "7337d681-0ede-49d8-b86e-55a455aaaca6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Resuming after Technician approval ===\n",
            "{'messages': [HumanMessage(content='Technician: Suggest flushing tank first before replacement.', additional_kwargs={}, response_metadata={})]}\n",
            "{'messages': [AIMessage(content='Several options depending on the context and desired level of detail:\\n\\n**Option 1 (Concise):**\\n\\n> \"Before replacing the [component], please flush the tank to remove any residual water or debris.\"\\n\\n**Option 2 (More detail):**\\n\\n> \"Prior to replacement, it\\'s recommended to flush the tank thoroughly. This will help prevent sediment or debris from entering the [component] during installation and ensure a cleaner working environment.\"\\n\\n**Option 3 (Most detail, including why):**\\n\\n> \"To minimize the risk of contamination and improve the installation process, please flush the tank completely before replacing the [component].  This removes any loose sediment or debris that could interfere with the new part\\'s function or cause damage during installation.  Ensure the tank is completely drained before proceeding.\"\\n\\n**Option 4 (For a specific component):**\\n\\n> \"Before replacing the [toilet] fill valve, please flush the tank thoroughly to remove any sediment that might clog the new valve.\"  (Replace \"[toilet] fill valve\" with the actual component)\\n\\n\\nRemember to replace \"[component]\" with the specific part being replaced (e.g., fill valve, flapper, chain).  The best option depends on your audience and the situation.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--d5d796ad-c431-46a2-a774-588596ece780-0', usage_metadata={'input_tokens': 10, 'output_tokens': 258, 'total_tokens': 268, 'input_token_details': {'cache_read': 0}})]}\n",
            "\n",
            "--- Final State ---\n",
            "AIMessage : Several options depending on the context and desired level of detail:\n",
            "\n",
            "**Option 1 (Concise):**\n",
            "\n",
            "> \"Before replacing the [component], please flush the tank to remove any residual water or debris.\"\n",
            "\n",
            "**Option 2 (More detail):**\n",
            "\n",
            "> \"Prior to replacement, it's recommended to flush the tank thoroughly. This will help prevent sediment or debris from entering the [component] during installation and ensure a cleaner working environment.\"\n",
            "\n",
            "**Option 3 (Most detail, including why):**\n",
            "\n",
            "> \"To minimize the risk of contamination and improve the installation process, please flush the tank completely before replacing the [component].  This removes any loose sediment or debris that could interfere with the new part's function or cause damage during installation.  Ensure the tank is completely drained before proceeding.\"\n",
            "\n",
            "**Option 4 (For a specific component):**\n",
            "\n",
            "> \"Before replacing the [toilet] fill valve, please flush the tank thoroughly to remove any sediment that might clog the new valve.\"  (Replace \"[toilet] fill valve\" with the actual component)\n",
            "\n",
            "\n",
            "Remember to replace \"[component]\" with the specific part being replaced (e.g., fill valve, flapper, chain).  The best option depends on your audience and the situation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pool.close()"
      ],
      "metadata": {
        "id": "jf7I9JhNYAaB"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}