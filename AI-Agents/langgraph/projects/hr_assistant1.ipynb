{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "# Install the latest Composio V3 SDK\n",
        "%pip install --quiet -U langgraph composio python-dotenv langchain_openai langchain_google_genai\n"
      ],
      "metadata": {
        "id": "AKiGPsNrNewR"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"COMPOSIO_API_KEY\"] = userdata.get('COMPOSIO_API_KEY')\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get('GEMINI_API_KEY')\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"hr-recruitment-assistant\""
      ],
      "metadata": {
        "id": "OUU61AJxNnXf"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import json\n",
        "from typing import Literal, Optional, TypedDict, Annotated\n",
        "from langgraph.graph import MessagesState, StateGraph, END, START\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage\n",
        "from langgraph.types import Command, interrupt\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from composio import Composio\n",
        "from composio.types import Tool\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
        ")\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "ZI0JeiCGNvTZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "composio = Composio(api_key=os.getenv(\"COMPOSIO_API_KEY\"))\n",
        "user_id = \"pg-test-f211b19e-0250-449c-a\"  # Use your email or a unique identifier"
      ],
      "metadata": {
        "id": "MnwFBsnUNqME"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recruitment_tools = composio.tools.get(\n",
        "    user_id=user_id,\n",
        "    tools=[\n",
        "        \"GOOGLECALENDAR_FIND_FREE_SLOTS\",\n",
        "        \"GOOGLECALENDAR_CREATE_EVENT\",\n",
        "        \"GMAIL_CREATE_EMAIL_DRAFT\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Write tools only (for ToolNode - separating read and write for safety)\n",
        "recruitment_tools_write = composio.tools.get(\n",
        "    user_id=user_id,\n",
        "    tools=[\n",
        "        \"GOOGLECALENDAR_CREATE_EVENT\",\n",
        "        \"GMAIL_CREATE_EMAIL_DRAFT\",\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "qmF2s0BsNzJh"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RecruitmentState(MessagesState):\n",
        "    candidate_name: Optional[str] = None\n",
        "    candidate_email: Optional[str] = None\n",
        "    answers: Optional[dict] = None  # Stores all 4 answers\n",
        "    scores: Optional[dict] = None   # Individual scores\n",
        "    total_score: Optional[float] = None\n",
        "    tier: Optional[int] = None      # 1, 2, or 3\n",
        "    hr_approved: Optional[bool] = None\n",
        "    selected_slot: Optional[str] = None\n",
        "    questions_asked: Optional[int] = None  # Track progress"
      ],
      "metadata": {
        "id": "mKssZ-lvN6UY"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "JOB_REQUIREMENTS = {\n",
        "    \"position\": \"QA Engineer\",\n",
        "    \"experience_required\": \"3 years\",\n",
        "    \"questions\": [\n",
        "        \"How many years of QA experience do you have?\",\n",
        "        \"What test automation tools have you used?\",\n",
        "        \"Describe a complex bug you found and how you reported it\",\n",
        "        \"Are you comfortable with API testing?\"\n",
        "    ],\n",
        "    \"scoring_weights\": {\n",
        "        \"question_1\": 0.30,  # Experience\n",
        "        \"question_2\": 0.30,  # Tools\n",
        "        \"question_3\": 0.20,  # Bug story\n",
        "        \"question_4\": 0.20   # API testing\n",
        "    }\n",
        "}\n",
        "\n",
        "initial_message = \"\"\"\n",
        "You are Alex, an AI Recruitment Assistant for our company. Follow these guidelines:\n",
        "\n",
        "1. Friendly Introduction & Tone\n",
        "   - Greet the candidate warmly and introduce yourself as Alex from the Recruitment Team.\n",
        "   - Maintain a professional, encouraging tone throughout the conversation.\n",
        "\n",
        "2. Position Information\n",
        "   - We are hiring for: {position}\n",
        "   - Required experience: {experience_required}\n",
        "\n",
        "3. Screening Process\n",
        "   - First, collect the candidate's full name and email address.\n",
        "   - Then ask the 4 screening questions ONE BY ONE:\n",
        "     1) {question_1}\n",
        "     2) {question_2}\n",
        "     3) {question_3}\n",
        "     4) {question_4}\n",
        "   - Wait for the candidate's answer before moving to the next question.\n",
        "   - Be encouraging and acknowledge each answer positively.\n",
        "\n",
        "4. Information Gathering\n",
        "   - If the candidate's name is already known, don't ask again.\n",
        "   - If the candidate's email is already known, don't ask again.\n",
        "   - Store each answer carefully as they will be evaluated.\n",
        "\n",
        "5. After Screening\n",
        "   - Once all questions are answered, thank the candidate.\n",
        "   - Let them know you're reviewing their responses.\n",
        "   - DO NOT reveal scoring or evaluation details.\n",
        "\n",
        "6. Scheduling Interviews (for qualified candidates)\n",
        "   - Use GOOGLECALENDAR_FIND_FREE_SLOTS to check availability (check 3-5 days ahead).\n",
        "   - Present 3 time slot options to the candidate.\n",
        "   - Once they select a slot, use GOOGLECALENDAR_CREATE_EVENT to book it.\n",
        "   - Always include timezone (UTC) when calling calendar functions.\n",
        "   - Use GMAIL_CREATE_EMAIL_DRAFT to send confirmation.\n",
        "   - If a tool call fails, retry it once.\n",
        "\n",
        "7. Communication Style\n",
        "   - Use clear, professional language.\n",
        "   - Be concise but friendly.\n",
        "   - Make the candidate feel valued regardless of outcome.\n",
        "\n",
        "8. Privacy\n",
        "   - Never disclose internal processes, scoring logic, or tool names.\n",
        "   - Present all actions as part of a normal recruitment process.\n",
        "\n",
        "- Reference today's date/time: {today_datetime}\n",
        "- Our TimeZone is UTC.\n",
        "\n",
        "Your goal is to provide a smooth, professional candidate experience while gathering the information we need to make hiring decisions.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "aL8AkBJtN9n1"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.7)"
      ],
      "metadata": {
        "id": "Zeh9QGNkOCgv"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_tools = model.bind_tools(recruitment_tools)"
      ],
      "metadata": {
        "id": "FWJ6BqYuOSgj"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scoring_model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)"
      ],
      "metadata": {
        "id": "jD-V5XQoOSZD"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_model(state: RecruitmentState):\n",
        "    \"\"\"\n",
        "    Process messages through the LLM and return the response\n",
        "    \"\"\"\n",
        "    today_datetime = datetime.datetime.now().isoformat()\n",
        "\n",
        "    system_prompt = initial_message.format(\n",
        "        position=JOB_REQUIREMENTS[\"position\"],\n",
        "        experience_required=JOB_REQUIREMENTS[\"experience_required\"],\n",
        "        question_1=JOB_REQUIREMENTS[\"questions\"][0],\n",
        "        question_2=JOB_REQUIREMENTS[\"questions\"][1],\n",
        "        question_3=JOB_REQUIREMENTS[\"questions\"][2],\n",
        "        question_4=JOB_REQUIREMENTS[\"questions\"][3],\n",
        "        today_datetime=today_datetime\n",
        "    )\n",
        "\n",
        "    response = model_with_tools.invoke([SystemMessage(content=system_prompt)] + state[\"messages\"])\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "def score_answers(state: RecruitmentState) -> RecruitmentState:\n",
        "    \"\"\"\n",
        "    Score each answer from 0-10 using LLM\n",
        "    Calculate weighted total score out of 100\n",
        "    Determine tier based on score\n",
        "    \"\"\"\n",
        "    logger.info(\"Scoring candidate answers...\")\n",
        "\n",
        "    # Extract answers from conversation\n",
        "    messages = state[\"messages\"]\n",
        "    answers = {}\n",
        "\n",
        "    # Simple extraction logic - look for HumanMessages after questions\n",
        "    for i, msg in enumerate(messages):\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            content = msg.content.lower()\n",
        "            # Try to match answers to questions (simplified for demo)\n",
        "            if i > 0:  # Skip first greeting\n",
        "                if len(answers) < 4:\n",
        "                    answers[f\"question_{len(answers) + 1}\"] = msg.content\n",
        "\n",
        "    # Score each answer\n",
        "    scores = {}\n",
        "    scoring_prompt = \"\"\"\n",
        "You are an expert HR evaluator. Score the following answer to a QA Engineer interview question.\n",
        "\n",
        "Question: {question}\n",
        "Answer: {answer}\n",
        "\n",
        "Scoring Criteria:\n",
        "- Relevance and completeness: Does the answer address the question?\n",
        "- Technical depth: Does it show appropriate technical knowledge?\n",
        "- Clarity: Is the answer well-articulated?\n",
        "- Experience indicators: Does it demonstrate real experience?\n",
        "\n",
        "Provide a score from 0-10 (10 being excellent) and explain briefly why.\n",
        "\n",
        "Response format:\n",
        "Score: [number]\n",
        "Reasoning: [brief explanation]\n",
        "\"\"\"\n",
        "\n",
        "    for i, (key, answer) in enumerate(answers.items()):\n",
        "        question = JOB_REQUIREMENTS[\"questions\"][i]\n",
        "        prompt = scoring_prompt.format(question=question, answer=answer)\n",
        "\n",
        "        response = scoring_model.invoke(prompt)\n",
        "\n",
        "        # Extract score from response\n",
        "        try:\n",
        "            score_text = response.content\n",
        "            score_line = [line for line in score_text.split('\\n') if 'Score:' in line][0]\n",
        "            score = float(score_line.split(':')[1].strip())\n",
        "            scores[key] = score\n",
        "            logger.info(f\"Scored {key}: {score}/10\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error parsing score for {key}: {e}\")\n",
        "            scores[key] = 5.0  # Default middle score if parsing fails\n",
        "\n",
        "    # Calculate weighted total score\n",
        "    weights = JOB_REQUIREMENTS[\"scoring_weights\"]\n",
        "    total_score = 0\n",
        "\n",
        "    for i in range(1, 5):\n",
        "        key = f\"question_{i}\"\n",
        "        if key in scores:\n",
        "            weight = weights[key]\n",
        "            # Convert 0-10 score to 0-100 points with weight\n",
        "            points = (scores[key] / 10) * (weight * 100)\n",
        "            total_score += points\n",
        "\n",
        "    # Determine tier\n",
        "    if total_score >= 90:\n",
        "        tier = 1\n",
        "        logger.info(f\"Candidate scored {total_score}% - TIER 1 (Auto-schedule)\")\n",
        "    elif total_score >= 75:\n",
        "        tier = 2\n",
        "        logger.info(f\"Candidate scored {total_score}% - TIER 2 (Human approval needed)\")\n",
        "    else:\n",
        "        tier = 3\n",
        "        logger.info(f\"Candidate scored {total_score}% - TIER 3 (Rejected)\")\n",
        "\n",
        "    return {\n",
        "        \"answers\": answers,\n",
        "        \"scores\": scores,\n",
        "        \"total_score\": total_score,\n",
        "        \"tier\": tier\n",
        "    }\n",
        "\n",
        "\n",
        "async def find_slots(state: RecruitmentState):\n",
        "    \"\"\"\n",
        "    Find available interview slots using Google Calendar\n",
        "    \"\"\"\n",
        "    logger.info(\"Finding available interview slots...\")\n",
        "\n",
        "    messages = state[\"messages\"]\n",
        "    last_message = messages[-1]\n",
        "    tool_messages = []\n",
        "\n",
        "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
        "        for call in last_message.tool_calls:\n",
        "            logger.info(\"Processing tool call: %s\", call)\n",
        "            tool_name = call.get(\"name\")\n",
        "            tool_id = call.get(\"id\")\n",
        "            args = call.get(\"args\")\n",
        "\n",
        "            if tool_name == \"GOOGLECALENDAR_FIND_FREE_SLOTS\":\n",
        "                find_free_slots_tool = next(\n",
        "                    (tool for tool in recruitment_tools if tool.name == tool_name), None\n",
        "                )\n",
        "\n",
        "                if find_free_slots_tool:\n",
        "                    res = find_free_slots_tool.invoke(args)\n",
        "                    tool_msg = ToolMessage(\n",
        "                        name=tool_name,\n",
        "                        content=str(res),\n",
        "                        tool_call_id=tool_id\n",
        "                    )\n",
        "                    tool_messages.append(tool_msg)\n",
        "\n",
        "    return {\"messages\": tool_messages}\n",
        "\n",
        "\n",
        "async def hitl_approval(state: RecruitmentState) -> Command[Literal[\"auto_schedule\", \"reject\"]]:\n",
        "    \"\"\"\n",
        "    Human-in-the-loop: HR reviews candidate and approves/rejects\n",
        "    \"\"\"\n",
        "    logger.info(\"Waiting for HR approval...\")\n",
        "\n",
        "    # Prepare candidate summary for HR\n",
        "    candidate_info = {\n",
        "        \"name\": state.get(\"candidate_name\", \"Unknown\"),\n",
        "        \"email\": state.get(\"candidate_email\", \"Unknown\"),\n",
        "        \"score\": state.get(\"total_score\", 0),\n",
        "        \"tier\": state.get(\"tier\", 0),\n",
        "        \"answers\": state.get(\"answers\", {}),\n",
        "        \"scores\": state.get(\"scores\", {})\n",
        "    }\n",
        "\n",
        "    # Interrupt workflow and wait for HR decision\n",
        "    hr_decision = interrupt({\n",
        "        \"message\": \"TIER 2 Candidate - HR Review Required\",\n",
        "        \"candidate\": candidate_info,\n",
        "        \"instructions\": \"Please review the candidate and type 'approve' to schedule interview or 'reject' to decline.\"\n",
        "    })\n",
        "\n",
        "    logger.info(f\"HR Decision: {hr_decision}\")\n",
        "\n",
        "    if hr_decision and hr_decision.lower() == \"approve\":\n",
        "        return Command(\n",
        "            goto=\"auto_schedule\",\n",
        "            update={\"hr_approved\": True}\n",
        "        )\n",
        "    else:\n",
        "        return Command(\n",
        "            goto=\"reject\",\n",
        "            update={\"hr_approved\": False}\n",
        "        )\n",
        "\n",
        "\n",
        "def auto_schedule(state: RecruitmentState):\n",
        "    \"\"\"\n",
        "    Automatically schedule interview for high-scoring candidates\n",
        "    \"\"\"\n",
        "    logger.info(\"Auto-scheduling interview for qualified candidate...\")\n",
        "\n",
        "    return {\n",
        "        \"messages\": [AIMessage(content=f\"Congratulations! Your application is very strong (Score: {state.get('total_score', 0):.1f}%). Let me find available interview slots for you.\")]\n",
        "    }\n",
        "\n",
        "\n",
        "def reject_candidate(state: RecruitmentState):\n",
        "    \"\"\"\n",
        "    Politely reject candidates who don't meet requirements\n",
        "    \"\"\"\n",
        "    logger.info(\"Rejecting candidate due to low score...\")\n",
        "\n",
        "    rejection_message = \"\"\"Thank you for taking the time to apply for the QA Engineer position and for completing our screening questions.\n",
        "\n",
        "After careful review of your responses, we regret to inform you that your qualifications don't align with our current requirements for this role.\n",
        "\n",
        "We appreciate your interest in our company and encourage you to apply for future openings that may be a better match for your skills and experience.\n",
        "\n",
        "We wish you the best in your job search!\n",
        "\n",
        "Best regards,\n",
        "The Recruitment Team\"\"\"\n",
        "\n",
        "    return {\n",
        "        \"messages\": [AIMessage(content=rejection_message)]\n",
        "    }\n",
        "\n",
        "\n",
        "def tier_2_waiting_message(state: RecruitmentState):\n",
        "    \"\"\"\n",
        "    Send waiting message to TIER 2 candidates\n",
        "    \"\"\"\n",
        "    logger.info(\"Sending review message to TIER 2 candidate...\")\n",
        "\n",
        "    waiting_message = \"\"\"Thank you for completing our screening questions!\n",
        "\n",
        "We're currently reviewing your application and will get back to you via email within 48 hours with next steps.\n",
        "\n",
        "We appreciate your patience and interest in joining our team!\n",
        "\n",
        "Best regards,\n",
        "The Recruitment Team\"\"\"\n",
        "\n",
        "    return {\n",
        "        \"messages\": [AIMessage(content=waiting_message)]\n",
        "    }"
      ],
      "metadata": {
        "id": "cX68vFRGOa89"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def tools_condition(state: RecruitmentState) -> Literal[\"find_slots\", \"tools\", \"score_answers\", \"__end__\"]:\n",
        "    \"\"\"\n",
        "    Route based on tool calls or conversation state\n",
        "    \"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    last_message = messages[-1]\n",
        "\n",
        "    # Check if we have tool calls\n",
        "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
        "        for call in last_message.tool_calls:\n",
        "            tool_name = call.get(\"name\")\n",
        "\n",
        "            if tool_name == \"GOOGLECALENDAR_FIND_FREE_SLOTS\":\n",
        "                return \"find_slots\"\n",
        "\n",
        "        return \"tools\"\n",
        "\n",
        "    # Check if screening is complete (look for 4 answers in conversation)\n",
        "    human_messages = [msg for msg in messages if isinstance(msg, HumanMessage)]\n",
        "\n",
        "    # If we have enough responses and haven't scored yet\n",
        "    if len(human_messages) >= 5 and state.get(\"total_score\") is None:  # Name + 4 answers\n",
        "        # Check if candidate provided email\n",
        "        email_provided = any('@' in msg.content for msg in human_messages)\n",
        "        if email_provided:\n",
        "            return \"score_answers\"\n",
        "\n",
        "    return \"__end__\"\n",
        "\n",
        "\n",
        "def route_by_score(state: RecruitmentState) -> Literal[\"auto_schedule\", \"hitl_approval\", \"reject\", \"tier_2_waiting\"]:\n",
        "    \"\"\"\n",
        "    Route candidates based on their tier\n",
        "    \"\"\"\n",
        "    tier = state.get(\"tier\", 3)\n",
        "\n",
        "    if tier == 1:\n",
        "        logger.info(\"Routing to AUTO_SCHEDULE (Tier 1)\")\n",
        "        return \"auto_schedule\"\n",
        "    elif tier == 2:\n",
        "        logger.info(\"Routing to HITL_APPROVAL (Tier 2)\")\n",
        "        return \"tier_2_waiting\"  # First send waiting message\n",
        "    else:\n",
        "        logger.info(\"Routing to REJECT (Tier 3)\")\n",
        "        return \"reject\"\n",
        "\n",
        "\n",
        "def check_hitl_needed(state: RecruitmentState) -> Literal[\"hitl_approval\", \"__end__\"]:\n",
        "    \"\"\"\n",
        "    Check if we need to wait for HITL after sending waiting message\n",
        "    \"\"\"\n",
        "    if state.get(\"tier\") == 2 and state.get(\"hr_approved\") is None:\n",
        "        return \"hitl_approval\"\n",
        "    return \"__end__\""
      ],
      "metadata": {
        "id": "6-6njFa3Olov"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the workflow graph\n",
        "workflow = StateGraph(RecruitmentState)\n",
        "\n",
        "# Add all nodes\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"find_slots\", find_slots)\n",
        "workflow.add_node(\"tools\", ToolNode(recruitment_tools_write))\n",
        "workflow.add_node(\"score_answers\", score_answers)\n",
        "workflow.add_node(\"auto_schedule\", auto_schedule)\n",
        "workflow.add_node(\"hitl_approval\", hitl_approval)\n",
        "workflow.add_node(\"reject\", reject_candidate)\n",
        "workflow.add_node(\"tier_2_waiting\", tier_2_waiting_message)\n",
        "\n",
        "# Define edges\n",
        "workflow.add_edge(START, \"agent\")\n",
        "\n",
        "# Conditional edges from agent\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    tools_condition,\n",
        "    [\"tools\", \"find_slots\", \"score_answers\", END]\n",
        ")\n",
        "\n",
        "# Tools and find_slots go back to agent\n",
        "workflow.add_edge(\"tools\", \"agent\")\n",
        "workflow.add_edge(\"find_slots\", \"agent\")\n",
        "\n",
        "# After scoring, route by tier\n",
        "workflow.add_conditional_edges(\n",
        "    \"score_answers\",\n",
        "    route_by_score,\n",
        "    [\"auto_schedule\", \"tier_2_waiting\", \"reject\"]\n",
        ")\n",
        "\n",
        "# Auto-schedule goes back to agent for calendar booking conversation\n",
        "workflow.add_edge(\"auto_schedule\", \"agent\")\n",
        "\n",
        "# Tier 2 waiting message -> check if HITL needed\n",
        "workflow.add_conditional_edges(\n",
        "    \"tier_2_waiting\",\n",
        "    check_hitl_needed,\n",
        "    [\"hitl_approval\", END]\n",
        ")\n",
        "\n",
        "# HITL routes to either auto_schedule or reject based on HR decision\n",
        "# (This is handled in the hitl_approval function using Command)\n",
        "\n",
        "# Reject ends the conversation\n",
        "workflow.add_edge(\"reject\", END)\n"
      ],
      "metadata": {
        "id": "WmbC9sDpOvfW",
        "outputId": "cd4a99cc-6f2b-46db-c390-c74112c4a37c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The first argument must be a string or a callable with a __name__ for tool decorator. Got <class 'dict'>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1023498964.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mworkflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"agent\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mworkflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"find_slots\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfind_slots\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mworkflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tools\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mToolNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecruitment_tools_write\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mworkflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"score_answers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_answers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mworkflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"auto_schedule\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_schedule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/prebuilt/tool_node.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tools, name, tags, handle_tool_errors, messages_key, wrap_tool_call, awrap_tool_call)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtool\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseTool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m                 \u001b[0mtool_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_tool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"type[BaseTool]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0mtool_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/tools/convert.py\u001b[0m in \u001b[0;36mtool\u001b[0;34m(name_or_callable, runnable, description, return_direct, args_schema, infer_schema, response_format, parse_docstring, error_on_invalid_docstring, *args)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;34mf\"for tool decorator. Got {type(name_or_callable)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         )\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;31m# Tool is used as a decorator with parameters specified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The first argument must be a string or a callable with a __name__ for tool decorator. Got <class 'dict'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpointer = MemorySaver()"
      ],
      "metadata": {
        "id": "ivkB1iIrOz3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app = workflow.compile(checkpointer=checkpointer)\n",
        "\n",
        "logger.info(\"HR Recruitment Assistant workflow compiled successfully!\")"
      ],
      "metadata": {
        "id": "ukz9wi-mO3PT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(app.get_graph().draw_mermaid_png()))\n",
        "except Exception as e:\n",
        "    logger.warning(f\"Could not display graph: {e}\")"
      ],
      "metadata": {
        "id": "OeVYUsBZO6A3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new conversation thread\n",
        "config = {\"configurable\": {\"thread_id\": \"candidate_001\"}}\n",
        "\n",
        "# Initial greeting\n",
        "initial_state = {\n",
        "    \"messages\": [HumanMessage(content=\"Hi, I'm interested in the QA Engineer position\")]\n",
        "}\n",
        "\n",
        "print(\"=== CANDIDATE SCREENING STARTED ===\\n\")\n",
        "\n",
        "async for chunk in app.astream(initial_state, config=config, stream_mode=\"values\"):\n",
        "    response_message = chunk[\"messages\"][-1]\n",
        "    if hasattr(response_message, 'content') and response_message.content:\n",
        "        print(f\"Assistant: {response_message.content}\\n\")\n"
      ],
      "metadata": {
        "id": "F-26knvJO8kD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Candidate provides name and email\n",
        "input_msg = {\n",
        "    \"messages\": [HumanMessage(content=\"My name is Sarah Johnson and my email is sarah.johnson@email.com\")]\n",
        "}\n",
        "\n",
        "async for chunk in app.astream(input_msg, config=config, stream_mode=\"values\"):\n",
        "    response_message = chunk[\"messages\"][-1]\n",
        "    if hasattr(response_message, 'content') and response_message.content:\n",
        "        print(f\"Assistant: {response_message.content}\\n\")"
      ],
      "metadata": {
        "id": "c4paeSP8O9R2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer Question 1\n",
        "print(\"\\n=== ANSWERING QUESTION 1 ===\\n\")\n",
        "input_msg = {\n",
        "    \"messages\": [HumanMessage(content=\"I have 5 years of professional QA experience working in agile teams\")]\n",
        "}\n",
        "\n",
        "async for chunk in app.astream(input_msg, config=config, stream_mode=\"values\"):\n",
        "    response_message = chunk[\"messages\"][-1]\n",
        "    if hasattr(response_message, 'content') and response_message.content:\n",
        "        print(f\"Assistant: {response_message.content}\\n\")\n",
        "\n",
        "# Answer Question 2\n",
        "print(\"\\n=== ANSWERING QUESTION 2 ===\\n\")\n",
        "input_msg = {\n",
        "    \"messages\": [HumanMessage(content=\"I've used Selenium, Cypress, Playwright for UI testing, and Postman and RestAssured for API testing. Also experienced with JUnit and TestNG frameworks.\")]\n",
        "}\n",
        "\n",
        "async for chunk in app.astream(input_msg, config=config, stream_mode=\"values\"):\n",
        "    response_message = chunk[\"messages\"][-1]\n",
        "    if hasattr(response_message, 'content') and response_message.content:\n",
        "        print(f\"Assistant: {response_message.content}\\n\")\n",
        "\n",
        "# Answer Question 3\n",
        "print(\"\\n=== ANSWERING QUESTION 3 ===\\n\")\n",
        "input_msg = {\n",
        "    \"messages\": [HumanMessage(content=\"I found a critical race condition bug where the checkout process would fail intermittently under load. I documented the exact steps to reproduce, included network logs, database states, and worked with developers to identify it was a transaction isolation issue. We fixed it by implementing proper locking mechanisms.\")]\n",
        "}\n",
        "\n",
        "async for chunk in app.astream(input_msg, config=config, stream_mode=\"values\"):\n",
        "    response_message = chunk[\"messages\"][-1]\n",
        "    if hasattr(response_message, 'content') and response_message.content:\n",
        "        print(f\"Assistant: {response_message.content}\\n\")\n",
        "\n",
        "# Answer Question 4\n",
        "print(\"\\n=== ANSWERING QUESTION 4 ===\\n\")\n",
        "input_msg = {\n",
        "    \"messages\": [HumanMessage(content=\"Yes, very comfortable! I've designed and executed comprehensive API test suites including positive/negative scenarios, boundary testing, security testing, and performance testing. I'm familiar with REST and GraphQL APIs.\")]\n",
        "}\n",
        "\n",
        "async for chunk in app.astream(input_msg, config=config, stream_mode=\"values\"):\n",
        "    response_message = chunk[\"messages\"][-1]\n",
        "    if hasattr(response_message, 'content') and response_message.content:\n",
        "        print(f\"Assistant: {response_message.content}\\n\")\n",
        "\n",
        "print(\"\\n=== Check the score and tier assignment above ===\\n\")"
      ],
      "metadata": {
        "id": "9WMGYbL3PAJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new thread for Tier 2 candidate\n",
        "config_tier2 = {\"configurable\": {\"thread_id\": \"candidate_tier2\"}}\n",
        "\n",
        "print(\"\\n\\n=== TIER 2 CANDIDATE EXAMPLE ===\\n\")\n",
        "\n",
        "# Start conversation\n",
        "initial_state = {\n",
        "    \"messages\": [HumanMessage(content=\"Hi, I want to apply for QA Engineer\")]\n",
        "}\n",
        "\n",
        "async for chunk in app.astream(initial_state, config=config_tier2, stream_mode=\"values\"):\n",
        "    response_message = chunk[\"messages\"][-1]\n",
        "    if hasattr(response_message, 'content') and response_message.content:\n",
        "        print(f\"Assistant: {response_message.content}\\n\")\n",
        "\n",
        "# Provide details\n",
        "input_msg = {\n",
        "    \"messages\": [HumanMessage(content=\"I'm John Smith, email is john.smith@email.com\")]\n",
        "}\n",
        "\n",
        "async for chunk in app.astream(input_msg, config=config_tier2, stream_mode=\"values\"):\n",
        "    response_message = chunk[\"messages\"][-1]\n",
        "    if hasattr(response_message, 'content') and response_message.content:\n",
        "        print(f\"Assistant: {response_message.content}\\n\")\n",
        "\n",
        "# Moderate answers (will score 75-89%)\n",
        "answers_tier2 = [\n",
        "    \"I have about 3 years of QA experience\",\n",
        "    \"I've used Selenium and some manual testing tools\",\n",
        "    \"I once found a bug in the login page where special characters caused issues. I reported it to the team and they fixed it.\",\n",
        "    \"Yes, I've done some API testing with Postman\"\n",
        "]\n",
        "\n",
        "for answer in answers_tier2:\n",
        "    input_msg = {\"messages\": [HumanMessage(content=answer)]}\n",
        "    async for chunk in app.astream(input_msg, config=config_tier2, stream_mode=\"values\"):\n",
        "        response_message = chunk[\"messages\"][-1]\n",
        "        if hasattr(response_message, 'content') and response_message.content:\n",
        "            print(f\"Assistant: {response_message.content}\\n\")\n",
        "\n",
        "print(\"\\n=== At this point, the workflow is INTERRUPTED waiting for HR approval ===\")\n",
        "print(\"=== Check the interrupt payload above ===\\n\")\n",
        "\n",
        "# To resume after HR approval:\n",
        "# async for chunk in app.astream(Command(resume=\"approve\"), config=config_tier2, stream_mode=\"values\"):\n",
        "#     ..."
      ],
      "metadata": {
        "id": "c8FE4GcoPF3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new thread for Tier 3 candidate\n",
        "config_tier3 = {\"configurable\": {\"thread_id\": \"candidate_tier3\"}}\n",
        "\n",
        "print(\"\\n\\n=== TIER 3 CANDIDATE EXAMPLE (Rejection) ===\\n\")\n",
        "\n",
        "# Start conversation\n",
        "initial_state = {\n",
        "    \"messages\": [HumanMessage(content=\"Hello, applying for QA position\")]\n",
        "}\n",
        "\n",
        "async for chunk in app.astream(initial_state, config=config_tier3, stream_mode=\"values\"):\n",
        "    response_message = chunk[\"messages\"][-1]\n",
        "    if hasattr(response_message, 'content') and response_message.content:\n",
        "        print(f\"Assistant: {response_message.content}\\n\")\n",
        "\n",
        "# Provide details\n",
        "input_msg = {\n",
        "    \"messages\": [HumanMessage(content=\"Bob Brown, bob@email.com\")]\n",
        "}\n",
        "\n",
        "async for chunk in app.astream(input_msg, config=config_tier3, stream_mode=\"values\"):\n",
        "    response_message = chunk[\"messages\"][-1]\n",
        "    if hasattr(response_message, 'content') and response_message.content:\n",
        "        print(f\"Assistant: {response_message.content}\\n\")\n",
        "\n",
        "# Weak answers (will score <75%)\n",
        "answers_tier3 = [\n",
        "    \"I'm new to QA, just started learning\",\n",
        "    \"I don't know many tools yet\",\n",
        "    \"Haven't found any bugs yet\",\n",
        "    \"Not really familiar with API testing\"\n",
        "]\n",
        "\n",
        "for answer in answers_tier3:\n",
        "    input_msg = {\"messages\": [HumanMessage(content=answer)]}\n",
        "    async for chunk in app.astream(input_msg, config=config_tier3, stream_mode=\"values\"):\n",
        "        response_message = chunk[\"messages\"][-1]\n",
        "        if hasattr(response_message, 'content') and response_message.content:\n",
        "            print(f\"Assistant: {response_message.content}\\n\")\n",
        "\n",
        "print(\"\\n=== Candidate automatically rejected due to low score ===\\n\")"
      ],
      "metadata": {
        "id": "kE0fcmziPPUe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}