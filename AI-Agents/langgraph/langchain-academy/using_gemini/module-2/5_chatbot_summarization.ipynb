{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b651ead9-5504-45ee-938d-f91ac78dddd1",
      "metadata": {
        "id": "b651ead9-5504-45ee-938d-f91ac78dddd1"
      },
      "source": [
        "# Chatbot with message summarization\n",
        "\n",
        "## Review\n",
        "\n",
        "We've covered how to customize graph state schema and reducer.\n",
        "\n",
        "We've also shown a number of ways to trim or filter messages in graph state.\n",
        "\n",
        "## Goals\n",
        "\n",
        "Now, let's take it one step further!\n",
        "\n",
        "Rather than just trimming or filtering messages, we'll show how to use LLMs to produce a running summary of the conversation.\n",
        "\n",
        "This allows us to retain a compressed representation of the full conversation, rather than just removing it with trimming or filtering.\n",
        "\n",
        "We'll incorporate this summarization into a simple Chatbot.  \n",
        "\n",
        "And we'll equip that Chatbot with memory, supporting long-running conversations without incurring high token cost / latency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "000a6daa-92ad-4e57-a060-d1c81176eb0d",
      "metadata": {
        "id": "000a6daa-92ad-4e57-a060-d1c81176eb0d"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langchain_core langgraph langchain_google_genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e39c0535",
      "metadata": {
        "id": "e39c0535"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "google_api_key = userdata.get('GEMINI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfddfce9-3a9b-4b35-a76d-28265515aabd",
      "metadata": {
        "id": "dfddfce9-3a9b-4b35-a76d-28265515aabd"
      },
      "source": [
        "We'll use [LangSmith](https://docs.smith.langchain.com/) for [tracing](https://docs.smith.langchain.com/concepts/tracing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "464856d4",
      "metadata": {
        "id": "464856d4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain-academy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6544ba0f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6544ba0f",
        "outputId": "52fa0444-9a36-420b-d549-a3d71fd97e56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n",
            "WARNING:langsmith.client:Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=61c3a72b-8b71-4389-825c-32dd1303bae3,id=61c3a72b-8b71-4389-825c-32dd1303bae3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hi there! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--61c3a72b-8b71-4389-825c-32dd1303bae3-0', usage_metadata={'input_tokens': 1, 'output_tokens': 11, 'total_tokens': 12, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "model: ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(model = \"gemini-1.5-flash\",api_key = google_api_key)\n",
        "model.invoke(\"hi\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db3afac3-8b7a-45db-a3c1-7e4125c1bc8b",
      "metadata": {
        "id": "db3afac3-8b7a-45db-a3c1-7e4125c1bc8b"
      },
      "source": [
        "We'll use `MessagesState`, as before.\n",
        "\n",
        "In addition to the built-in `messages` key, we'll now include a custom key (`summary`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "948e60f0-5c76-4235-b40e-cf523205d40e",
      "metadata": {
        "id": "948e60f0-5c76-4235-b40e-cf523205d40e"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import MessagesState\n",
        "\n",
        "\n",
        "class State(MessagesState):\n",
        "    summary: str"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6855ea31-5cc1-4277-a189-0b72459f67ec",
      "metadata": {
        "id": "6855ea31-5cc1-4277-a189-0b72459f67ec"
      },
      "source": [
        "We'll define a node to call our LLM that incorporates a summary, if it exists, into the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c3f7d19b-afe0-4381-9b1a-0a832b162e7b",
      "metadata": {
        "id": "c3f7d19b-afe0-4381-9b1a-0a832b162e7b"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
        "\n",
        "# Define the logic to call the model\n",
        "def call_model(state: State) -> State:\n",
        "\n",
        "    # Get summary if it exists and if it not exists then it return empty string\n",
        "    summary = state.get(\"summary\", \"\")\n",
        "\n",
        "    # If there is summary, then we add it\n",
        "    if summary:\n",
        "\n",
        "        # Add summary to system message\n",
        "        system_message = f\"Summary of conversation earlier: {summary}\"\n",
        "\n",
        "        # Append summary to any newer messages\n",
        "        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
        "\n",
        "    else:\n",
        "        messages = state[\"messages\"]\n",
        "\n",
        "    response = model.invoke(messages)\n",
        "    return {\"messages\": response}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6882042c-b42d-4d52-a6a7-6ec8efa72450",
      "metadata": {
        "id": "6882042c-b42d-4d52-a6a7-6ec8efa72450"
      },
      "source": [
        "We'll define a node to produce a summary.\n",
        "\n",
        "Note, here we'll use `RemoveMessage` to filter our state after we've produced the summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "78c7aa59-3760-4e76-93f1-bc713e3ec39e",
      "metadata": {
        "id": "78c7aa59-3760-4e76-93f1-bc713e3ec39e"
      },
      "outputs": [],
      "source": [
        "def summarize_conversation(state: State):\n",
        "\n",
        "    # First, we get any existing summary\n",
        "    summary = state.get(\"summary\", \"\")\n",
        "    # If \"summary\" is present in state, it returns the stored summary string.\n",
        "    # If \"summary\" is missing, it returns \"\" (an empty string) instead of throwing a KeyError.\n",
        "\n",
        "    # Create our summarization prompt\n",
        "    if summary:\n",
        "\n",
        "        # A summary already exists\n",
        "        summary_message = (\n",
        "            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n",
        "            \"Extend the summary by taking into account the new messages above:\"\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        summary_message = \"Create a summary of the conversation above:\"\n",
        "\n",
        "    # Add prompt to our history\n",
        "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
        "    response = model.invoke(messages)\n",
        "\n",
        "    # Delete all but the 2 most recent messages\n",
        "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
        "    return {\"summary\": response.content, \"messages\": delete_messages}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f982993e-f4be-4ff7-9a38-886f75398b3d",
      "metadata": {
        "id": "f982993e-f4be-4ff7-9a38-886f75398b3d"
      },
      "source": [
        "We'll add a conditional edge to determine whether to produce a summary based on the conversation length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b507665d-7f5d-442a-b498-218c94c5dd8b",
      "metadata": {
        "id": "b507665d-7f5d-442a-b498-218c94c5dd8b"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import END\n",
        "# Determine whether to end or summarize the conversation\n",
        "def should_continue(state: State):\n",
        "\n",
        "    \"\"\"Return the next node to execute.\"\"\"\n",
        "\n",
        "    messages = state[\"messages\"]\n",
        "\n",
        "    # If there are more than six messages, then we summarize the conversation\n",
        "    if len(messages) > 6:\n",
        "        return \"summarize_conversation\"\n",
        "\n",
        "    # Otherwise we can just end\n",
        "    return END"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a838f4c-7067-4f7f-a4c4-6654e11214cd",
      "metadata": {
        "id": "5a838f4c-7067-4f7f-a4c4-6654e11214cd"
      },
      "source": [
        "## Adding memory\n",
        "\n",
        "Recall that [state is transient](https://github.com/langchain-ai/langgraph/discussions/352#discussioncomment-9291220) to a single graph execution.\n",
        "\n",
        "This limits our ability to have multi-turn conversations with interruptions.\n",
        "\n",
        "As introduced at the end of Module 1, we can use [persistence](https://langchain-ai.github.io/langgraph/how-tos/persistence/) to address this!\n",
        "\n",
        "LangGraph can use a checkpointer to automatically save the graph state after each step.\n",
        "\n",
        "This built-in persistence layer gives us memory, allowing LangGraph to pick up from the last state update.\n",
        "\n",
        "As we previously showed, one of the easiest to work with is `MemorySaver`, an in-memory key-value store for Graph state.\n",
        "\n",
        "All we need to do is compile the graph with a checkpointer, and our graph has memory!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1d57516d-f9f1-4d3c-a84a-7277b5ce6df6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "1d57516d-f9f1-4d3c-a84a-7277b5ce6df6",
        "outputId": "bdd54dc4-939a-433e-a9a6-109fbb000bc3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import StateGraph, START\n",
        "from langgraph.graph.state import CompiledStateGraph\n",
        "\n",
        "# Define a new graph\n",
        "workflow: StateGraph = StateGraph(State)\n",
        "workflow.add_node(\"conversation\", call_model)\n",
        "workflow.add_node(summarize_conversation)\n",
        "\n",
        "# Set the entrypoint as conversation\n",
        "workflow.add_edge(START, \"conversation\")\n",
        "workflow.add_conditional_edges(\"conversation\", should_continue)\n",
        "workflow.add_edge(\"summarize_conversation\", END)\n",
        "\n",
        "# Compile\n",
        "memory: MemorySaver = MemorySaver()\n",
        "graph: CompiledStateGraph = workflow.compile(checkpointer=memory)\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0bd5d23-ac3b-4496-a049-9a9f97d2feb9",
      "metadata": {
        "id": "d0bd5d23-ac3b-4496-a049-9a9f97d2feb9"
      },
      "source": [
        "## Threads\n",
        "\n",
        "The checkpointer saves the state at each step as a checkpoint.\n",
        "\n",
        "These saved checkpoints can be grouped into a `thread` of conversation.\n",
        "\n",
        "Think about Slack as an analog: different channels carry different conversations.\n",
        "\n",
        "Threads are like Slack channels, capturing grouped collections of state (e.g., conversation).\n",
        "\n",
        "Below, we use `configurable` to set a thread ID.\n",
        "\n",
        "![state.jpg](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbadf3b379c2ee621adfd1_chatbot-summarization1.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2566c93b-13e6-4a53-bc0f-b00fff691d30",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2566c93b-13e6-4a53-bc0f-b00fff691d30",
        "outputId": "b07f4de1-cd8c-4dec-e780-d8e22b17f53a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hi Lance! Nice to meet you.  How can I help you today?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Your name is Lance.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "That's great!  Go Niners!  Are you looking forward to the upcoming season?\n"
          ]
        }
      ],
      "source": [
        "# Create a thread\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "# Start conversation\n",
        "input_message = HumanMessage(content=\"hi! I'm Lance\")\n",
        "output = graph.invoke({\"messages\": [input_message]}, config)\n",
        "for m in output['messages'][-1:]:\n",
        "    m.pretty_print()\n",
        "\n",
        "input_message = HumanMessage(content=\"what's my name?\")\n",
        "output = graph.invoke({\"messages\": [input_message]}, config)\n",
        "for m in output['messages'][-1:]:\n",
        "    m.pretty_print()\n",
        "\n",
        "input_message = HumanMessage(content=\"i like the 49ers!\")\n",
        "output = graph.invoke({\"messages\": [input_message]}, config)\n",
        "for m in output['messages'][-1:]:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "531e5b63-5e8b-486e-baa0-a45521e2fbc2",
      "metadata": {
        "id": "531e5b63-5e8b-486e-baa0-a45521e2fbc2"
      },
      "source": [
        "Now, we don't yet have a summary of the state because we still have < = 6 messages.\n",
        "\n",
        "This was set in `should_continue`.\n",
        "\n",
        "```\n",
        "    # If there are more than six messages, then we summarize the conversation\n",
        "    if len(messages) > 6:\n",
        "        return \"summarize_conversation\"\n",
        "```\n",
        "\n",
        "We can pick up the conversation because we have the thread."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "91b82aaa-17f9-49e2-9528-f4b22e23ebcb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "91b82aaa-17f9-49e2-9528-f4b22e23ebcb",
        "outputId": "eaac6f43-eb2a-4eba-8feb-fb46699e48cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "graph.get_state(config).values.get(\"summary\",\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "068a93e9-f716-4980-8edf-94115017d865",
      "metadata": {
        "id": "068a93e9-f716-4980-8edf-94115017d865"
      },
      "source": [
        "The `config` with thread ID allows us to proceed from the previously logged state!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "24b34f0f-62ef-4008-8e96-480cbe92ea3e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24b34f0f-62ef-4008-8e96-480cbe92ea3e",
        "outputId": "c85a75df-1ebf-4a45-defb-e6e47554005b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=15d43dfd-a17d-41f7-ae17-93640c02fcbf,id=15d43dfd-a17d-41f7-ae17-93640c02fcbf; trace=15d43dfd-a17d-41f7-ae17-93640c02fcbf,id=ed242690-9b5c-41c6-a3ab-94c094d3bbe5; trace=15d43dfd-a17d-41f7-ae17-93640c02fcbf,id=27273ba1-f335-4210-ab34-9ebcedbea50e\n",
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=15d43dfd-a17d-41f7-ae17-93640c02fcbf,id=27273ba1-f335-4210-ab34-9ebcedbea50e; trace=15d43dfd-a17d-41f7-ae17-93640c02fcbf,id=b2103312-38be-4396-b922-e9f626ed4399; trace=15d43dfd-a17d-41f7-ae17-93640c02fcbf,id=b2103312-38be-4396-b922-e9f626ed4399; trace=15d43dfd-a17d-41f7-ae17-93640c02fcbf,id=ed242690-9b5c-41c6-a3ab-94c094d3bbe5; trace=15d43dfd-a17d-41f7-ae17-93640c02fcbf,id=a64d3b45-57a6-46d2-ba1c-e025770e6f9c; trace=15d43dfd-a17d-41f7-ae17-93640c02fcbf,id=8cc0b6b3-9390-4799-a6ba-13ec781bc596\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Nick Bosa is incredibly talented, and he *was* the highest-paid defensive player in the NFL for a while after signing his massive contract extension.  However,  depending on the exact timing of when you ask,  that might have changed with other players signing new deals.  The highest-paid player title in the NFL shifts frequently with new contracts.\n"
          ]
        }
      ],
      "source": [
        "input_message = HumanMessage(content=\"i like Nick Bosa, isn't he the highest paid defensive player?\")\n",
        "output = graph.invoke({\"messages\": [input_message]}, config)\n",
        "for m in output['messages'][-1:]:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "id": "2PDdzspUfbRH",
        "outputId": "60665dae-be89-4db4-8d6f-1b6b2b9ffda4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2PDdzspUfbRH",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content=\"i like Nick Bosa, isn't he the highest paid defensive player?\", additional_kwargs={}, response_metadata={}, id='a88ea0e8-2950-413d-a987-a3ebf899cab7'),\n",
              "  AIMessage(content='Nick Bosa is incredibly talented, and he *was* the highest-paid defensive player in the NFL for a while after signing his massive contract extension.  However,  depending on the exact timing of when you ask,  that might have changed with other players signing new deals.  The highest-paid player title in the NFL shifts frequently with new contracts.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--27273ba1-f335-4210-ab34-9ebcedbea50e-0', usage_metadata={'input_tokens': 77, 'output_tokens': 74, 'total_tokens': 151, 'input_token_details': {'cache_read': 0}})],\n",
              " 'summary': 'The conversation began with introductions, where I identified the user as Lance.  Lance then expressed his fandom of the San Francisco 49ers and specifically mentioned his appreciation for Nick Bosa, questioning whether Bosa holds the title of highest-paid defensive player. I responded affirmatively, but with the caveat that this is a fluid situation subject to change with new NFL contracts.'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output['messages']"
      ],
      "metadata": {
        "id": "vxuPB3EijfiP",
        "outputId": "540a46e8-4cb6-4198-a836-fd08481282ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "vxuPB3EijfiP",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content=\"i like Nick Bosa, isn't he the highest paid defensive player?\", additional_kwargs={}, response_metadata={}, id='a88ea0e8-2950-413d-a987-a3ebf899cab7'),\n",
              " AIMessage(content='Nick Bosa is incredibly talented, and he *was* the highest-paid defensive player in the NFL for a while after signing his massive contract extension.  However,  depending on the exact timing of when you ask,  that might have changed with other players signing new deals.  The highest-paid player title in the NFL shifts frequently with new contracts.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--27273ba1-f335-4210-ab34-9ebcedbea50e-0', usage_metadata={'input_tokens': 77, 'output_tokens': 74, 'total_tokens': 151, 'input_token_details': {'cache_read': 0}})]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output['summary']"
      ],
      "metadata": {
        "id": "H43ElWwseizr",
        "outputId": "bfc8c80b-a475-4239-abbd-fe0e79a6299c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "id": "H43ElWwseizr",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The conversation began with introductions, where I identified the user as Lance.  Lance then expressed his fandom of the San Francisco 49ers and specifically mentioned his appreciation for Nick Bosa, questioning whether Bosa holds the title of highest-paid defensive player. I responded affirmatively, but with the caveat that this is a fluid situation subject to change with new NFL contracts.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graph.get_state(config)"
      ],
      "metadata": {
        "id": "A6roYu5Ef88K",
        "outputId": "8556bae3-15c9-49cc-c871-41374d7ba477",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "A6roYu5Ef88K",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StateSnapshot(values={'messages': [HumanMessage(content=\"i like Nick Bosa, isn't he the highest paid defensive player?\", additional_kwargs={}, response_metadata={}, id='a88ea0e8-2950-413d-a987-a3ebf899cab7'), AIMessage(content='Nick Bosa is incredibly talented, and he *was* the highest-paid defensive player in the NFL for a while after signing his massive contract extension.  However,  depending on the exact timing of when you ask,  that might have changed with other players signing new deals.  The highest-paid player title in the NFL shifts frequently with new contracts.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--27273ba1-f335-4210-ab34-9ebcedbea50e-0', usage_metadata={'input_tokens': 77, 'output_tokens': 74, 'total_tokens': 151, 'input_token_details': {'cache_read': 0}})], 'summary': 'The conversation began with introductions, where I identified the user as Lance.  Lance then expressed his fandom of the San Francisco 49ers and specifically mentioned his appreciation for Nick Bosa, questioning whether Bosa holds the title of highest-paid defensive player. I responded affirmatively, but with the caveat that this is a fluid situation subject to change with new NFL contracts.'}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0875ed-8b0d-6653-800b-455d4b79b145'}}, metadata={'source': 'loop', 'step': 11, 'parents': {}}, created_at='2025-09-01T18:09:48.062863+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0875ed-82e9-6f3a-800a-35b2d7731b05'}}, tasks=(), interrupts=())"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graph.get_state(config).values[\"messages\"]"
      ],
      "metadata": {
        "id": "LA4oKblNl_YJ",
        "outputId": "be593b89-3b67-45ac-c81c-81b6bc61b05e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "LA4oKblNl_YJ",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content=\"i like Nick Bosa, isn't he the highest paid defensive player?\", additional_kwargs={}, response_metadata={}, id='a88ea0e8-2950-413d-a987-a3ebf899cab7'),\n",
              " AIMessage(content='Nick Bosa is incredibly talented, and he *was* the highest-paid defensive player in the NFL for a while after signing his massive contract extension.  However,  depending on the exact timing of when you ask,  that might have changed with other players signing new deals.  The highest-paid player title in the NFL shifts frequently with new contracts.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--27273ba1-f335-4210-ab34-9ebcedbea50e-0', usage_metadata={'input_tokens': 77, 'output_tokens': 74, 'total_tokens': 151, 'input_token_details': {'cache_read': 0}})]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graph.get_state(config).values[\"summary\"]"
      ],
      "metadata": {
        "id": "HmSGfRIRlqJb",
        "outputId": "e075d93a-cdd9-4dd7-a07b-cc8dce0b90ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "id": "HmSGfRIRlqJb",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The conversation began with introductions, where I identified the user as Lance.  Lance then expressed his fandom of the San Francisco 49ers and specifically mentioned his appreciation for Nick Bosa, questioning whether Bosa holds the title of highest-paid defensive player. I responded affirmatively, but with the caveat that this is a fluid situation subject to change with new NFL contracts.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "22f1b35f-e4bb-47f6-87b1-d84d8aed9aa9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "22f1b35f-e4bb-47f6-87b1-d84d8aed9aa9",
        "outputId": "17f61714-b08d-41b9-ff54-6da4cb6ad3d8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The conversation began with introductions, where I identified the user as Lance.  Lance then expressed his fandom of the San Francisco 49ers and specifically mentioned his appreciation for Nick Bosa, questioning whether Bosa holds the title of highest-paid defensive player. I responded affirmatively, but with the caveat that this is a fluid situation subject to change with new NFL contracts.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "graph.get_state(config).values.get(\"summary\",\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad7cc0ab-905a-4037-b7cb-69db5b89591e",
      "metadata": {
        "id": "ad7cc0ab-905a-4037-b7cb-69db5b89591e"
      },
      "source": [
        "## LangSmith\n",
        "\n",
        "Let's review the trace!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}