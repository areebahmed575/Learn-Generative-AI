{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot with message summarization & external DB memory\n",
        "\n",
        "## Review\n",
        "\n",
        "We've covered how to customize graph state schema and reducer.\n",
        "\n",
        "We've also shown a number of tricks for trimming or filtering messages in graph state.\n",
        "\n",
        "We've used these concepts in a Chatbot with memory that produces a running summary of the conversation.\n",
        "\n",
        "## Goals\n",
        "\n",
        "But, what if we want our Chatbot to have memory that persists indefinitely?\n",
        "\n",
        "Now, we'll introduce some more advanced checkpointers that support external databases.\n",
        "\n",
        "Here, we'll show how to use [Postgres as a checkpointer](https://langchain-ai.github.io/langgraph/how-tos/persistence_postgres/)"
      ],
      "metadata": {
        "id": "9iKKYdAkVoxu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Lwi6YkLu31aK"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U langgraph langgraph-checkpoint-postgres psycopg psycopg-pool langchain_google_genai\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "i6Tn95wS4rxz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain-academy\""
      ],
      "metadata": {
        "id": "qtL0cG1B8pMt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use sync connectionÂ¶\n",
        "This sets up a synchronous connection to the database.\n",
        "\n",
        "Synchronous connections execute operations in a blocking manner, meaning each operation waits for completion before moving to the next one. The DB_URI is the database connection URI, with the protocol used for connecting to a PostgreSQL database, authentication, and host where database is running. The connection_kwargs dictionary defines additional parameters for the database connection."
      ],
      "metadata": {
        "id": "AqAjciKmV6vl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "DB_URI = userdata.get('DB_URI')"
      ],
      "metadata": {
        "id": "cP2TdPSB3-dO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from psycopg_pool import ConnectionPool\n",
        "from langgraph.checkpoint.postgres import PostgresSaver\n",
        "\n",
        "# Connection pool for efficient database access\n",
        "connection_kwargs = {\"autocommit\": True, \"prepare_threshold\": 0}\n",
        "\n",
        "# Create a persistent connection pool\n",
        "pool = ConnectionPool(conninfo=DB_URI, max_size=20, kwargs=connection_kwargs)\n",
        "\n",
        "# Initialize PostgresSaver checkpointer\n",
        "checkpointer = PostgresSaver(pool)\n",
        "checkpointer.setup()  # Ensure database tables are set up\n"
      ],
      "metadata": {
        "id": "0XCqHjWM4LTc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's re-define our chatbot."
      ],
      "metadata": {
        "id": "w9All8mCV1o0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
        "\n",
        "from langgraph.graph import END\n",
        "from langgraph.graph import MessagesState\n",
        "\n",
        "model: ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(model = \"gemini-1.5-flash\", api_key =  GEMINI_API_KEY)\n",
        "\n",
        "class State(MessagesState):\n",
        "    summary: str\n",
        "\n",
        "# Define the logic to call the model\n",
        "def call_model(state: State) -> State:\n",
        "\n",
        "    # Get summary if it exists\n",
        "    summary = state.get(\"summary\", \"\")\n",
        "    print(f\"Using summary: {summary}\")\n",
        "\n",
        "    # If there is summary, then we add it\n",
        "    if summary:\n",
        "\n",
        "        # Add summary to system message\n",
        "        system_message = f\"Summary of conversation earlier: {summary}\"\n",
        "\n",
        "        # Append summary to any newer messages\n",
        "        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
        "\n",
        "    else:\n",
        "        messages = state[\"messages\"]\n",
        "\n",
        "    response = model.invoke(messages)\n",
        "    return {\"messages\": response}\n",
        "\n",
        "def summarize_conversation(state: State) -> State:\n",
        "    print(f\"Messages before summarizing: {len(state['messages'])}\")\n",
        "    # First, we get any existing summary\n",
        "    summary = state.get(\"summary\", \"\")\n",
        "    print(f\"Existing summary: {summary}\")\n",
        "\n",
        "    # Create our summarization prompt\n",
        "    if summary:\n",
        "\n",
        "        # A summary already exists\n",
        "        summary_message = (\n",
        "            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n",
        "            \"Extend the summary by taking into account the new messages above:\"\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        summary_message = \"Create a summary of the conversation above:\"\n",
        "\n",
        "\n",
        "    # Add prompt to our history\n",
        "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
        "    response = model.invoke(messages)\n",
        "    # Summarization logic\n",
        "    print(f\"New summary: {response.content}\")\n",
        "\n",
        "    # Delete all but the 2 most recent messages\n",
        "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
        "\n",
        "    print(f\"Messages after truncation: {len(delete_messages)}\")\n",
        "    return {\"summary\": response.content, \"messages\": delete_messages}\n",
        "\n",
        "# Determine whether to end or summarize the conversation\n",
        "def should_continue(state: State) -> State:\n",
        "\n",
        "    \"\"\"Return the next node to execute.\"\"\"\n",
        "\n",
        "    messages = state[\"messages\"]\n",
        "    print(f\"Message count: {len(messages)}\")\n",
        "    # If there are more than six messages, then we summarize the conversation\n",
        "    if len(messages) > 6:\n",
        "        return \"summarize_conversation\"\n",
        "\n",
        "    # Otherwise we can just end\n",
        "    return END"
      ],
      "metadata": {
        "id": "kvU-4FnS4Wxu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we just re-compile with our postgres checkpointer."
      ],
      "metadata": {
        "id": "zzIVvGsXWap4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.state import CompiledStateGraph\n",
        "\n",
        "# Redefine workflow\n",
        "workflow = StateGraph(State)\n",
        "workflow.add_node(\"conversation\", call_model)\n",
        "workflow.add_node(summarize_conversation)\n",
        "\n",
        "workflow.add_edge(START, \"conversation\")\n",
        "workflow.add_conditional_edges(\"conversation\", should_continue)\n",
        "workflow.add_edge(\"summarize_conversation\", END)\n",
        "\n",
        "# Compile the workflow with PostgreSQL checkpointer\n",
        "graph = workflow.compile(checkpointer=checkpointer)\n"
      ],
      "metadata": {
        "id": "d7wrnazV4mdo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can invoke the graph several times."
      ],
      "metadata": {
        "id": "C_KTIXuvWkT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration for thread\n",
        "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
        "\n",
        "# Start a conversation\n",
        "input_message = HumanMessage(content=\"hi! I'm Areeb\")\n",
        "output = graph.invoke({\"messages\": [input_message]}, config)\n",
        "for m in output['messages'][-1:]:\n",
        "    m.pretty_print()\n",
        "\n",
        "# Check the persisted state\n",
        "graph_state = graph.get_state(config)\n",
        "graph_state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wk129iSWZapB",
        "outputId": "33549580-8236-4789-d01f-851a31780172"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using summary: \n",
            "Message count: 2\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hi Areeb!  Nice to meet you. How can I help you today?\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StateSnapshot(values={'messages': [HumanMessage(content=\"hi! I'm Areeb\", additional_kwargs={}, response_metadata={}, id='dfb4c33c-465e-4fac-9668-9382148e5e94'), AIMessage(content='Hi Areeb!  Nice to meet you. How can I help you today?\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-b6efa33a-3dcb-417b-a2d0-112853f40ade-0', usage_metadata={'input_tokens': 9, 'output_tokens': 19, 'total_tokens': 28, 'input_token_details': {'cache_read': 0}})]}, next=(), config={'configurable': {'thread_id': '4', 'checkpoint_ns': '', 'checkpoint_id': '1efbafc2-25b3-610e-8001-06275904d0e0'}}, metadata={'step': 1, 'source': 'loop', 'writes': {'conversation': {'messages': AIMessage(content='Hi Areeb!  Nice to meet you. How can I help you today?\\n', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'safety_ratings': [], 'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}}, id='run-b6efa33a-3dcb-417b-a2d0-112853f40ade-0', usage_metadata={'input_tokens': 9, 'output_tokens': 19, 'total_tokens': 28, 'input_token_details': {'cache_read': 0}})}}, 'parents': {}, 'thread_id': '4'}, created_at='2024-12-15T15:49:14.270086+00:00', parent_config={'configurable': {'thread_id': '4', 'checkpoint_ns': '', 'checkpoint_id': '1efbafc2-1dfc-621b-8000-82160cd3005a'}}, tasks=())"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration for thread\n",
        "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
        "\n",
        "# Start a conversation\n",
        "input_message = HumanMessage(content=\"I like painting pictures.\")\n",
        "output = graph.invoke({\"messages\": [input_message]}, config)\n",
        "for m in output['messages'][-1:]:\n",
        "    m.pretty_print()\n",
        "\n",
        "# Check the persisted state\n",
        "graph_state = graph.get_state(config)\n",
        "graph_state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxWCVPkuZvf-",
        "outputId": "ba5b4602-a936-4569-d4d2-2faaf50fce0d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using summary: \n",
            "Message count: 4\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "That's wonderful, Areeb!  What kind of pictures do you like to paint?  Do you have a favorite subject or style?\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StateSnapshot(values={'messages': [HumanMessage(content=\"hi! I'm Areeb\", additional_kwargs={}, response_metadata={}, id='dfb4c33c-465e-4fac-9668-9382148e5e94'), AIMessage(content='Hi Areeb!  Nice to meet you. How can I help you today?\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-b6efa33a-3dcb-417b-a2d0-112853f40ade-0', usage_metadata={'input_tokens': 9, 'output_tokens': 19, 'total_tokens': 28, 'input_token_details': {'cache_read': 0}}), HumanMessage(content='I like painting pictures.', additional_kwargs={}, response_metadata={}, id='a9eb58f5-b7ab-4461-9197-a330629cb82f'), AIMessage(content=\"That's wonderful, Areeb!  What kind of pictures do you like to paint?  Do you have a favorite subject or style?\\n\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-71f7de44-de7a-482b-988a-feb77d3ebb05-0', usage_metadata={'input_tokens': 35, 'output_tokens': 31, 'total_tokens': 66, 'input_token_details': {'cache_read': 0}})]}, next=(), config={'configurable': {'thread_id': '4', 'checkpoint_ns': '', 'checkpoint_id': '1efbafc2-f060-6e2a-8004-4f301231da33'}}, metadata={'step': 4, 'source': 'loop', 'writes': {'conversation': {'messages': AIMessage(content=\"That's wonderful, Areeb!  What kind of pictures do you like to paint?  Do you have a favorite subject or style?\\n\", additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'safety_ratings': [], 'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}}, id='run-71f7de44-de7a-482b-988a-feb77d3ebb05-0', usage_metadata={'input_tokens': 35, 'output_tokens': 31, 'total_tokens': 66, 'input_token_details': {'cache_read': 0}})}}, 'parents': {}, 'thread_id': '4'}, created_at='2024-12-15T15:49:35.522523+00:00', parent_config={'configurable': {'thread_id': '4', 'checkpoint_ns': '', 'checkpoint_id': '1efbafc2-ea8f-65db-8003-20eb3570c77c'}}, tasks=())"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After that disconnect your runtime and run until you compile your graph.Then,run below code to check whether  state is persisted or not"
      ],
      "metadata": {
        "id": "oCkXCdaECuCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration for thread\n",
        "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
        "\n",
        "# Start a conversation\n",
        "input_message = HumanMessage(content=\"What's my name and what is my hobby?\")\n",
        "output = graph.invoke({\"messages\": [input_message]}, config)\n",
        "for m in output['messages'][-1:]:\n",
        "    m.pretty_print()\n",
        "\n",
        "# Check the persisted state\n",
        "graph_state = graph.get_state(config)\n",
        "graph_state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxLLjBiBfZAL",
        "outputId": "b439b41b-d88c-4fac-ff28-97cebc13a4a0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using summary: \n",
            "Message count: 6\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Your name is Areeb, and your hobby is painting pictures.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StateSnapshot(values={'messages': [HumanMessage(content=\"hi! I'm Areeb\", additional_kwargs={}, response_metadata={}, id='dfb4c33c-465e-4fac-9668-9382148e5e94'), AIMessage(content='Hi Areeb!  Nice to meet you. How can I help you today?\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-b6efa33a-3dcb-417b-a2d0-112853f40ade-0', usage_metadata={'input_tokens': 9, 'output_tokens': 19, 'total_tokens': 28, 'input_token_details': {'cache_read': 0}}), HumanMessage(content='I like painting pictures.', additional_kwargs={}, response_metadata={}, id='a9eb58f5-b7ab-4461-9197-a330629cb82f'), AIMessage(content=\"That's wonderful, Areeb!  What kind of pictures do you like to paint?  Do you have a favorite subject or style?\\n\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-71f7de44-de7a-482b-988a-feb77d3ebb05-0', usage_metadata={'input_tokens': 35, 'output_tokens': 31, 'total_tokens': 66, 'input_token_details': {'cache_read': 0}}), HumanMessage(content=\"What's my name and what is my hobby?\", additional_kwargs={}, response_metadata={}, id='d9b9cbc6-612c-43db-9ae3-4c3014424f10'), AIMessage(content='Your name is Areeb, and your hobby is painting pictures.\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-505c55fd-5e86-4a81-86e2-cc742ffc2f52-0', usage_metadata={'input_tokens': 79, 'output_tokens': 15, 'total_tokens': 94, 'input_token_details': {'cache_read': 0}})]}, next=(), config={'configurable': {'thread_id': '4', 'checkpoint_ns': '', 'checkpoint_id': '1efbafcc-f42e-6d6f-8007-34a36da4c373'}}, metadata={'step': 7, 'source': 'loop', 'writes': {'conversation': {'messages': AIMessage(content='Your name is Areeb, and your hobby is painting pictures.\\n', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'safety_ratings': [], 'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}}, id='run-505c55fd-5e86-4a81-86e2-cc742ffc2f52-0', usage_metadata={'input_tokens': 79, 'output_tokens': 15, 'total_tokens': 94, 'input_token_details': {'cache_read': 0}})}}, 'parents': {}, 'thread_id': '4'}, created_at='2024-12-15T15:54:04.356929+00:00', parent_config={'configurable': {'thread_id': '4', 'checkpoint_ns': '', 'checkpoint_id': '1efbafcc-e90e-6cca-8006-447413c47eee'}}, tasks=())"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pool.close()"
      ],
      "metadata": {
        "id": "d_YbgTBs89_o"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Persisting state\n",
        "\n",
        "Using database like Postgres means state is persisted!\n",
        "\n",
        "For example, we can re-start the notebook kernel and see that we can still load from Postgres DB on disk.\n"
      ],
      "metadata": {
        "id": "Io-K7MCWinYF"
      }
    }
  ]
}