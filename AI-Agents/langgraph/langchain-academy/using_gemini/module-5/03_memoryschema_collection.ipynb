{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyK-PTxM3Nwr"
      },
      "source": [
        "# Chatbot with Collection Schema\n",
        "\n",
        "## Review\n",
        "\n",
        "We extended our chatbot to save semantic memories to a single [user profile](https://langchain-ai.github.io/langgraph/concepts/memory/#profile).\n",
        "\n",
        "We also introduced a library, [Trustcall](https://github.com/hinthornw/trustcall), to update this schema with new information.\n",
        "\n",
        "## Goals\n",
        "\n",
        "Sometimes we want to save memories to a [collection](https://docs.google.com/presentation/d/181mvjlgsnxudQI6S3ritg9sooNyu4AcLLFH1UK0kIuk/edit#slide=id.g30eb3c8cf10_0_200) rather than single profile.\n",
        "\n",
        "Here we'll update our chatbot to [save memories to a collection](https://langchain-ai.github.io/langgraph/concepts/memory/#collection).\n",
        "\n",
        "We'll also show how to use [Trustcall](https://github.com/hinthornw/trustcall) to update this collection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-aQWMFje3Nwu"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U langchain_openai langgraph trustcall langchain_core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZXT-nB2h3Nwv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain-academy\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKz9M10L3Nwv"
      },
      "source": [
        "## Defining a collection schema\n",
        "\n",
        "Instead of storing user information in a fixed profile structure, we'll create a flexible collection schema to store memories about user interactions.\n",
        "\n",
        "Each memory will be stored as a separate entry with a single `content` field for the main information we want to remember\n",
        "\n",
        "This approach allows us to build an open-ended collection of memories that can grow and change as we learn more about the user.\n",
        "\n",
        "We can define a collection schema as a [Pydantic](https://docs.pydantic.dev/latest/) object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Q28IhZCU3Nwv"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class Memory(BaseModel):\n",
        "    content: str = Field(description=\"The main content of the memory. For example: User expressed interest in learning about French.\")\n",
        "\n",
        "class MemoryCollection(BaseModel):\n",
        "    memories: list[Memory] = Field(description=\"A list of memories about the user.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TN2mCWww3Nww"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8Qw3x6d3Nww"
      },
      "source": [
        "We can used LangChain's chat model [chat model](https://python.langchain.com/docs/concepts/chat_models/) interface's [`with_structured_output`](https://python.langchain.com/docs/concepts/structured_outputs/#recommended-usage) method to enforce structured output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EZmQiUk13Nww",
        "outputId": "a26b7844-03e3-4dd5-d43a-78a1fd64c023",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MemoryCollection(memories=[Memory(content=\"User's name is Lance.\"), Memory(content='User enjoys biking.')])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Initialize the model\n",
        "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# Bind schema to model\n",
        "model_with_structure = model.with_structured_output(MemoryCollection)\n",
        "\n",
        "# Invoke the model to produce structured output that matches the schema\n",
        "memory_collection = model_with_structure.invoke([HumanMessage(\"My name is Lance. I like to bike.\")])\n",
        "memory_collection"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory_collection.memories"
      ],
      "metadata": {
        "id": "d9PJ8gva7GCc",
        "outputId": "d7aed9ab-94d9-4801-a00d-af56a0f4b52e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Memory(content=\"User's name is Lance.\"),\n",
              " Memory(content='User enjoys biking.')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vd51MhMN3Nwx"
      },
      "source": [
        "We can use `model_dump()` to serialize a Pydantic model instance into a Python dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "u0Ymnyh33Nwy",
        "outputId": "c6ce46f2-2423-4645-e866-406dd267b284",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'content': \"User's name is Lance.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "memory_collection.memories[0].model_dump()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcx6kD_p3Nwy"
      },
      "source": [
        "Save dictionary representation of each memory to the store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3V-rVs8A3Nwy"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "from langgraph.store.memory import InMemoryStore\n",
        "\n",
        "# Initialize the in-memory store\n",
        "in_memory_store = InMemoryStore()\n",
        "\n",
        "# Namespace for the memory to save\n",
        "user_id = \"1\"\n",
        "namespace_for_memory = (user_id, \"memories\")\n",
        "\n",
        "# Save a memory to namespace as key and value\n",
        "key = str(uuid.uuid4())\n",
        "value = memory_collection.memories[0].model_dump()\n",
        "in_memory_store.put(namespace_for_memory, key, value)\n",
        "\n",
        "key = str(uuid.uuid4())\n",
        "value = memory_collection.memories[1].model_dump()\n",
        "in_memory_store.put(namespace_for_memory, key, value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhPCwvrV3Nwy"
      },
      "source": [
        "Search for memories in the store."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "in_memory_store.search(namespace_for_memory)"
      ],
      "metadata": {
        "id": "cnpFoJ6A7_mF",
        "outputId": "dd110b70-1026-4287-bb15-5f282c8cf20e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Item(namespace=['1', 'memories'], key='65c6b44e-2d25-4ee6-b9bb-d8ea70856158', value={'content': \"User's name is Lance.\"}, created_at='2025-10-20T20:00:18.786149+00:00', updated_at='2025-10-20T20:00:18.786152+00:00', score=None),\n",
              " Item(namespace=['1', 'memories'], key='0ab556f1-751c-4e05-ae45-346553e9c0d4', value={'content': 'User enjoys biking.'}, created_at='2025-10-20T20:00:18.786341+00:00', updated_at='2025-10-20T20:00:18.786343+00:00', score=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Search\n",
        "for m in in_memory_store.search(namespace_for_memory):\n",
        "    print(m)"
      ],
      "metadata": {
        "id": "1yqgQno78XKx",
        "outputId": "69bbae15-6cb2-4136-8cd0-6cb8014651c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Item(namespace=['1', 'memories'], key='65c6b44e-2d25-4ee6-b9bb-d8ea70856158', value={'content': \"User's name is Lance.\"}, created_at='2025-10-20T20:00:18.786149+00:00', updated_at='2025-10-20T20:00:18.786152+00:00', score=None)\n",
            "Item(namespace=['1', 'memories'], key='0ab556f1-751c-4e05-ae45-346553e9c0d4', value={'content': 'User enjoys biking.'}, created_at='2025-10-20T20:00:18.786341+00:00', updated_at='2025-10-20T20:00:18.786343+00:00', score=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "55_uHNiL3Nwy",
        "outputId": "636c8373-7955-4d82-e830-685202a44611",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'namespace': ['1', 'memories'], 'key': '65c6b44e-2d25-4ee6-b9bb-d8ea70856158', 'value': {'content': \"User's name is Lance.\"}, 'created_at': '2025-10-20T20:00:18.786149+00:00', 'updated_at': '2025-10-20T20:00:18.786152+00:00', 'score': None}\n",
            "{'namespace': ['1', 'memories'], 'key': '0ab556f1-751c-4e05-ae45-346553e9c0d4', 'value': {'content': 'User enjoys biking.'}, 'created_at': '2025-10-20T20:00:18.786341+00:00', 'updated_at': '2025-10-20T20:00:18.786343+00:00', 'score': None}\n"
          ]
        }
      ],
      "source": [
        "# Search\n",
        "for m in in_memory_store.search(namespace_for_memory):\n",
        "    print(m.dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf_Czw6G3Nwz"
      },
      "source": [
        "## Updating collection schema\n",
        "\n",
        "We discussed the challenges with updating a profile schema in the last lesson.\n",
        "\n",
        "The same applies for collections!\n",
        "\n",
        "We want the ability to update the collection with new memories as well as update existing memories in the collection.\n",
        "\n",
        "Now we'll show that [Trustcall](https://github.com/hinthornw/trustcall) can be also used to update a collection.\n",
        "\n",
        "This enables both addition of new memories as well as [updating existing memories in the collection](https://github.com/hinthornw/trustcall?tab=readme-ov-file#simultanous-updates--insertions\n",
        ").\n",
        "\n",
        "Let's define a new extractor with Trustcall.\n",
        "\n",
        "As before, we provide the schema for each memory, `Memory`.  \n",
        "\n",
        "But, we can supply `enable_inserts=True` to allow the extractor to insert new memories to the collection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7omsfQOW3Nwz"
      },
      "outputs": [],
      "source": [
        "from trustcall import create_extractor\n",
        "\n",
        "# Create the extractor\n",
        "trustcall_extractor = create_extractor(\n",
        "    model,\n",
        "    tools=[Memory],\n",
        "    tool_choice=\"Memory\",\n",
        "    enable_inserts=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "owUCY1i33Nwz"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "\n",
        "# Instruction\n",
        "instruction = \"\"\"Extract memories from the following conversation:\"\"\"\n",
        "\n",
        "# Conversation\n",
        "conversation = [HumanMessage(content=\"Hi, I'm Lance.\"),\n",
        "                AIMessage(content=\"Nice to meet you, Lance.\"),\n",
        "                HumanMessage(content=\"This morning I had a nice bike ride in San Francisco.\")]\n",
        "\n",
        "# Invoke the extractor\n",
        "result = trustcall_extractor.invoke({\"messages\": [SystemMessage(content=instruction)] + conversation})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "lhqfW5TRAJjC",
        "outputId": "515c42e1-91e7-4f84-e2b2-6f987235b4ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 108, 'total_tokens': 125, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb3c3cb84d', 'id': 'chatcmpl-CSqLLJiF4Mfumxyw4VYc8QCTn1rMy', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--5d440058-a99c-4d1b-a243-bf1acb0bad8f-0', tool_calls=[{'name': 'Memory', 'args': {'content': 'Lance had a nice bike ride in San Francisco this morning.'}, 'id': 'call_PF3q5KvO6AhpYsPpvvxWOHcD', 'type': 'tool_call'}], usage_metadata={'input_tokens': 108, 'output_tokens': 17, 'total_tokens': 125, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})],\n",
              " 'responses': [Memory(content='Lance had a nice bike ride in San Francisco this morning.')],\n",
              " 'response_metadata': [{'id': 'call_PF3q5KvO6AhpYsPpvvxWOHcD'}],\n",
              " 'attempts': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "s2AcHRFV3Nwz",
        "outputId": "b1cdc662-d481-4b22-91b3-aac382146991",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  Memory (call_PF3q5KvO6AhpYsPpvvxWOHcD)\n",
            " Call ID: call_PF3q5KvO6AhpYsPpvvxWOHcD\n",
            "  Args:\n",
            "    content: Lance had a nice bike ride in San Francisco this morning.\n"
          ]
        }
      ],
      "source": [
        "# Messages contain the tool calls\n",
        "for m in result[\"messages\"]:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "YPNmB-Ib3Nwz",
        "outputId": "e22a35b8-f6af-4bec-ca0c-701a2c56e49b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Lance had a nice bike ride in San Francisco this morning.'\n"
          ]
        }
      ],
      "source": [
        "# Responses contain the memories that adhere to the schema\n",
        "for m in result[\"responses\"]:\n",
        "    print(m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "x-Dn7LIS3Nw0",
        "outputId": "3e7872d0-cbcd-4b74-abef-9c6a38ab07f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'call_PF3q5KvO6AhpYsPpvvxWOHcD'}\n"
          ]
        }
      ],
      "source": [
        "# Metadata contains the tool call\n",
        "for m in result[\"response_metadata\"]:\n",
        "    print(m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ymaiMaK13Nw0",
        "outputId": "967a3c37-ed59-4d32-f09b-25ecd38afc19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('0',\n",
              "  'Memory',\n",
              "  {'content': 'Lance had a nice bike ride in San Francisco this morning.'})]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Update the conversation\n",
        "updated_conversation = [AIMessage(content=\"That's great, did you do after?\"),\n",
        "                        HumanMessage(content=\"I went to Tartine and ate a croissant.\"),\n",
        "                        AIMessage(content=\"What else is on your mind?\"),\n",
        "                        HumanMessage(content=\"I was thinking about my Japan, and going back this winter!\"),]\n",
        "\n",
        "# Update the instruction\n",
        "system_msg = \"\"\"Update existing memories and create new ones based on the following conversation:\"\"\"\n",
        "\n",
        "# We'll save existing memories, giving them an ID, key (tool name), and value\n",
        "tool_name = \"Memory\"\n",
        "existing_memories = [(str(i), tool_name, memory.model_dump()) for i, memory in enumerate(result[\"responses\"])] if result[\"responses\"] else None\n",
        "existing_memories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "XqYBw2Ww3Nw0"
      },
      "outputs": [],
      "source": [
        "# Invoke the extractor with our updated conversation and existing memories\n",
        "result = trustcall_extractor.invoke({\"messages\": updated_conversation,\n",
        "                                     \"existing\": existing_memories})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "vopzyAibCS4n",
        "outputId": "463dbbdd-32e0-4452-ffa6-0840cecb766d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [AIMessage(content='', additional_kwargs={'updated_docs': {'call_w1oaocF0XxF5KGipibLQ5OnP': '0'}}, response_metadata={}, id='06c274cd-abce-4300-bed2-00bd15c815e6', tool_calls=[{'name': 'Memory', 'args': {'content': ' After the ride, Lance went to Tartine and ate a croissant. He was also thinking about his trip to Japan and planning to go back this winter.'}, 'id': 'call_w1oaocF0XxF5KGipibLQ5OnP', 'type': 'tool_call'}, {'name': 'Memory', 'args': {'content': 'Lance went to Tartine and ate a croissant after his bike ride. He was also thinking about his trip to Japan and planning to go back this winter.'}, 'id': 'call_hNEubIwgp34ATMzm0RAbQ0F2', 'type': 'tool_call'}])],\n",
              " 'responses': [Memory(content=' After the ride, Lance went to Tartine and ate a croissant. He was also thinking about his trip to Japan and planning to go back this winter.'),\n",
              "  Memory(content='Lance went to Tartine and ate a croissant after his bike ride. He was also thinking about his trip to Japan and planning to go back this winter.')],\n",
              " 'response_metadata': [{'id': 'call_w1oaocF0XxF5KGipibLQ5OnP',\n",
              "   'json_doc_id': '0'},\n",
              "  {'id': 'call_hNEubIwgp34ATMzm0RAbQ0F2'}],\n",
              " 'attempts': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "OrDjoNEW3Nw1",
        "outputId": "daf07963-6aa2-4835-fd4c-bac7de7885a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  Memory (call_w1oaocF0XxF5KGipibLQ5OnP)\n",
            " Call ID: call_w1oaocF0XxF5KGipibLQ5OnP\n",
            "  Args:\n",
            "    content:  After the ride, Lance went to Tartine and ate a croissant. He was also thinking about his trip to Japan and planning to go back this winter.\n",
            "  Memory (call_hNEubIwgp34ATMzm0RAbQ0F2)\n",
            " Call ID: call_hNEubIwgp34ATMzm0RAbQ0F2\n",
            "  Args:\n",
            "    content: Lance went to Tartine and ate a croissant after his bike ride. He was also thinking about his trip to Japan and planning to go back this winter.\n"
          ]
        }
      ],
      "source": [
        "# Messages from the model indicate two tool calls were made\n",
        "for m in result[\"messages\"]:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1Ya8xCek3Nw1",
        "outputId": "aeb78c3f-86d2-4008-8273-c1ed52647ae2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content=' After the ride, Lance went to Tartine and ate a croissant. He was also thinking about his trip to Japan and planning to go back this winter.'\n",
            "content='Lance went to Tartine and ate a croissant after his bike ride. He was also thinking about his trip to Japan and planning to go back this winter.'\n"
          ]
        }
      ],
      "source": [
        "# Responses contain the memories that adhere to the schema\n",
        "for m in result[\"responses\"]:\n",
        "    print(m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhtCM1jj3Nw1"
      },
      "source": [
        "This tells us that we updated the first memory in the collection by specifying the `json_doc_id`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "0grjY6Ik3Nw1",
        "outputId": "a277eb5f-514c-4f56-a5b8-6d18ba2f1c1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'call_w1oaocF0XxF5KGipibLQ5OnP', 'json_doc_id': '0'}\n",
            "{'id': 'call_hNEubIwgp34ATMzm0RAbQ0F2'}\n"
          ]
        }
      ],
      "source": [
        "# Metadata contains the tool call\n",
        "for m in result[\"response_metadata\"]:\n",
        "    print(m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzX_Ra4h3Nw2"
      },
      "source": [
        "LangSmith trace:\n",
        "\n",
        "https://smith.langchain.com/public/ebc1cb01-f021-4794-80c0-c75d6ea90446/r"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IATlG_r03Nw2"
      },
      "source": [
        "## Chatbot with collection schema updating\n",
        "\n",
        "Now, let's bring Trustcall into our chatbot to create and update a memory collection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "5MkJaZ9n3Nw2",
        "outputId": "319a9cda-a8d1-458d-a37a-9c9290d09f0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "import uuid\n",
        "\n",
        "from langgraph.graph import StateGraph, MessagesState, START, END\n",
        "from langgraph.store.memory import InMemoryStore\n",
        "from langchain_core.messages import merge_message_runs\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_core.runnables.config import RunnableConfig\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.store.base import BaseStore\n",
        "\n",
        "# Initialize the model\n",
        "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# Memory schema\n",
        "class Memory(BaseModel):\n",
        "    content: str = Field(description=\"The main content of the memory. For example: User expressed interest in learning about French.\")\n",
        "\n",
        "# Create the Trustcall extractor\n",
        "trustcall_extractor = create_extractor(\n",
        "    model,\n",
        "    tools=[Memory],\n",
        "    tool_choice=\"Memory\",\n",
        "    # This allows the extractor to insert new memories\n",
        "    enable_inserts=True,\n",
        ")\n",
        "\n",
        "# Chatbot instruction\n",
        "MODEL_SYSTEM_MESSAGE = \"\"\"You are a helpful chatbot. You are designed to be a companion to a user.\n",
        "\n",
        "You have a long term memory which keeps track of information you learn about the user over time.\n",
        "\n",
        "Current Memory (may include updated memories from this conversation):\n",
        "\n",
        "{memory}\"\"\"\n",
        "\n",
        "# Trustcall instruction\n",
        "TRUSTCALL_INSTRUCTION = \"\"\"Reflect on following interaction.\n",
        "\n",
        "Use the provided tools to retain any necessary memories about the user.\n",
        "\n",
        "Use parallel tool calling to handle updates and insertions simultaneously:\"\"\"\n",
        "\n",
        "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
        "\n",
        "    \"\"\"Load memories from the store and use them to personalize the chatbot's response.\"\"\"\n",
        "\n",
        "    # Get the user ID from the config\n",
        "    user_id = config[\"configurable\"][\"user_id\"]\n",
        "\n",
        "    # Retrieve memory from the store\n",
        "    namespace = (\"memories\", user_id)\n",
        "    memories = store.search(namespace)\n",
        "\n",
        "    # Format the memories for the system prompt\n",
        "    info = \"\\n\".join(f\"- {mem.value['content']}\" for mem in memories)\n",
        "    system_msg = MODEL_SYSTEM_MESSAGE.format(memory=info)\n",
        "\n",
        "    # Respond using memory as well as the chat history\n",
        "    response = model.invoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
        "\n",
        "    return {\"messages\": response}\n",
        "\n",
        "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
        "\n",
        "    \"\"\"Reflect on the chat history and update the memory collection.\"\"\"\n",
        "\n",
        "    # Get the user ID from the config\n",
        "    user_id = config[\"configurable\"][\"user_id\"]\n",
        "\n",
        "    # Define the namespace for the memories\n",
        "    namespace = (\"memories\", user_id)\n",
        "\n",
        "    # Retrieve the most recent memories for context\n",
        "    existing_items = store.search(namespace)\n",
        "\n",
        "    # Format the existing memories for the Trustcall extractor\n",
        "    tool_name = \"Memory\"\n",
        "    existing_memories = ([(existing_item.key, tool_name, existing_item.value)\n",
        "                          for existing_item in existing_items]\n",
        "                          if existing_items\n",
        "                          else None\n",
        "                        )\n",
        "\n",
        "    # Merge the chat history and the instruction\n",
        "    updated_messages=list(merge_message_runs(messages=[SystemMessage(content=TRUSTCALL_INSTRUCTION)] + state[\"messages\"]))\n",
        "\n",
        "    # Invoke the extractor\n",
        "    result = trustcall_extractor.invoke({\"messages\": updated_messages,\n",
        "                                        \"existing\": existing_memories})\n",
        "\n",
        "    # Save the memories from Trustcall to the store\n",
        "    for r, rmeta in zip(result[\"responses\"], result[\"response_metadata\"]):\n",
        "        store.put(namespace,\n",
        "                  rmeta.get(\"json_doc_id\", str(uuid.uuid4())),\n",
        "                  r.model_dump(mode=\"json\"),\n",
        "            )\n",
        "\n",
        "# Define the graph\n",
        "builder = StateGraph(MessagesState)\n",
        "builder.add_node(\"call_model\", call_model)\n",
        "builder.add_node(\"write_memory\", write_memory)\n",
        "builder.add_edge(START, \"call_model\")\n",
        "builder.add_edge(\"call_model\", \"write_memory\")\n",
        "builder.add_edge(\"write_memory\", END)\n",
        "\n",
        "# Store for long-term (across-thread) memory\n",
        "across_thread_memory = InMemoryStore()\n",
        "\n",
        "# Checkpointer for short-term (within-thread) memory\n",
        "within_thread_memory = MemorySaver()\n",
        "\n",
        "# Compile the graph with the checkpointer fir and store\n",
        "graph = builder.compile(checkpointer=within_thread_memory, store=across_thread_memory)\n",
        "\n",
        "# View\n",
        "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "m012ty7G3Nw3",
        "outputId": "91ae613b-bf4d-4f6b-f26e-ffe657041c90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Hi, my name is Lance\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hi Lance! It's great to meet you. How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "# We supply a thread ID for short-term (within-thread) memory\n",
        "# We supply a user ID for long-term (across-thread) memory\n",
        "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
        "\n",
        "# User input\n",
        "input_messages = [HumanMessage(content=\"Hi, my name is Lance\")]\n",
        "\n",
        "# Run the graph\n",
        "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
        "    chunk[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "-mKFOb6p3Nw3",
        "outputId": "c3a006df-cf05-4ff5-bcda-89e5e2291f3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "I like to bike around San Francisco\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "That sounds like a lot of fun! San Francisco has some beautiful routes for biking. Do you have a favorite trail or area you like to explore?\n"
          ]
        }
      ],
      "source": [
        "# User input\n",
        "input_messages = [HumanMessage(content=\"I like to bike around San Francisco\")]\n",
        "\n",
        "# Run the graph\n",
        "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
        "    chunk[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Z0g5BHrO3Nw3",
        "outputId": "bc5132e5-bb10-4916-b014-81f65d0b7651",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'namespace': ['memories', '1'], 'key': 'bc674a8a-1dd8-4229-a9d4-6c6d722b0424', 'value': {'content': 'User likes to bike around San Francisco.'}, 'created_at': '2025-10-20T20:01:06.868321+00:00', 'updated_at': '2025-10-20T20:01:06.868322+00:00', 'score': None}\n",
            "{'namespace': ['memories', '1'], 'key': '0f7f9f77-722a-4d6d-8eb5-98eb1cf6d108', 'value': {'content': 'User likes to bike around San Francisco.'}, 'created_at': '2025-10-20T20:01:06.868295+00:00', 'updated_at': '2025-10-20T20:01:06.868296+00:00', 'score': None}\n"
          ]
        }
      ],
      "source": [
        "# Namespace for the memory to save\n",
        "user_id = \"1\"\n",
        "namespace = (\"memories\", user_id)\n",
        "memories = across_thread_memory.search(namespace)\n",
        "for m in memories:\n",
        "    print(m.dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "IV_gJP-J3Nw3",
        "outputId": "fbd35566-b3ca-4983-85a6-4c820ba1672a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "I also enjoy going to bakeries\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Biking and bakeries make a great combination! Do you have a favorite bakery in San Francisco, or are you on the lookout for new ones to try?\n"
          ]
        }
      ],
      "source": [
        "# User input\n",
        "input_messages = [HumanMessage(content=\"I also enjoy going to bakeries\")]\n",
        "\n",
        "# Run the graph\n",
        "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
        "    chunk[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3834j0PT3Nw4"
      },
      "source": [
        "Continue the conversation in a new thread."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "BYyvY8Ev3Nw4",
        "outputId": "e17dfca5-55b9-4538-b60b-43e2f65a507c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What bakeries do you recommend for me?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Since you enjoy biking around San Francisco, you might like to visit some of these bakeries:\n",
            "\n",
            "1. **Tartine Bakery** - Located in the Mission District, it's a great spot to enjoy some delicious pastries and bread. Plus, the area is bike-friendly.\n",
            "\n",
            "2. **Arizmendi Bakery** - This worker-owned cooperative in the Inner Sunset offers a variety of baked goods, and it's a nice ride through Golden Gate Park to get there.\n",
            "\n",
            "3. **B. Patisserie** - Situated in Lower Pacific Heights, this bakery is known for its kouign-amann and other French pastries. It's a bit of a climb, but the treats are worth it!\n",
            "\n",
            "4. **Mr. Holmes Bakehouse** - Famous for their cruffins, this Tenderloin bakery is a fun stop, and you can explore the nearby neighborhoods on your bike.\n",
            "\n",
            "5. **Noe Valley Bakery** - A cozy spot in Noe Valley, perfect for a sweet treat after biking through the charming streets of the area.\n",
            "\n",
            "Do any of these sound like a good destination for your next ride?\n"
          ]
        }
      ],
      "source": [
        "# We supply a thread ID for short-term (within-thread) memory\n",
        "# We supply a user ID for long-term (across-thread) memory\n",
        "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
        "\n",
        "# User input\n",
        "input_messages = [HumanMessage(content=\"What bakeries do you recommend for me?\")]\n",
        "\n",
        "# Run the graph\n",
        "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
        "    chunk[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Namespace for the memory to save\n",
        "user_id = \"1\"\n",
        "namespace = (\"memories\", user_id)\n",
        "memories = across_thread_memory.search(namespace)\n",
        "for m in memories:\n",
        "    print(m.dict())"
      ],
      "metadata": {
        "id": "Mwn-WWJrHvlC",
        "outputId": "d4f679e8-129c-43f9-eda4-72749fc9d425",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'namespace': ['memories', '1'], 'key': 'bc674a8a-1dd8-4229-a9d4-6c6d722b0424', 'value': {'content': 'User likes to bike around San Francisco.'}, 'created_at': '2025-10-20T20:01:34.474507+00:00', 'updated_at': '2025-10-20T20:01:34.474508+00:00', 'score': None}\n",
            "{'namespace': ['memories', '1'], 'key': '0f7f9f77-722a-4d6d-8eb5-98eb1cf6d108', 'value': {'content': 'User likes to bike around San Francisco.'}, 'created_at': '2025-10-20T20:01:34.474545+00:00', 'updated_at': '2025-10-20T20:01:34.474546+00:00', 'score': None}\n",
            "{'namespace': ['memories', '1'], 'key': 'b43873ee-2d03-49c3-82a8-de0acc532d7d', 'value': {'content': 'Lance enjoys going to bakeries.'}, 'created_at': '2025-10-20T20:01:34.474468+00:00', 'updated_at': '2025-10-20T20:01:34.474470+00:00', 'score': None}\n",
            "{'namespace': ['memories', '1'], 'key': 'b1ad9799-49fd-4943-8d58-9a7c89efeba5', 'value': {'content': 'User is looking for bakery recommendations in San Francisco.'}, 'created_at': '2025-10-20T20:01:40.986433+00:00', 'updated_at': '2025-10-20T20:01:40.986435+00:00', 'score': None}\n",
            "{'namespace': ['memories', '1'], 'key': '3e513c70-c425-4b0f-aca0-56fd36fb1891', 'value': {'content': 'User enjoys biking and is interested in visiting bakeries in San Francisco.'}, 'created_at': '2025-10-20T20:01:40.986472+00:00', 'updated_at': '2025-10-20T20:01:40.986473+00:00', 'score': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We supply a thread ID for short-term (within-thread) memory\n",
        "# We supply a user ID for long-term (across-thread) memory\n",
        "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
        "\n",
        "# User input\n",
        "input_messages = [HumanMessage(content=\"What do you know about me.My name and interest etc?\")]\n",
        "\n",
        "# Run the graph\n",
        "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
        "    chunk[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "id": "mfe0NEPHNxAs",
        "outputId": "13e30725-e7d5-4235-8f18-747f63f2ff11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What do you know about me.My name and interest etc?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "From our previous conversations, I know that you enjoy biking around San Francisco and have an interest in visiting bakeries. Your name is Lance, and you like exploring different bakeries in the city. If there's anything else you'd like to share or update, feel free to let me know!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSWR00K93Nw4"
      },
      "source": [
        "### LangSmith\n",
        "\n",
        "https://smith.langchain.com/public/c87543ec-b426-4a82-a3ab-94d01c01d9f4/r\n",
        "\n",
        "## Studio\n",
        "\n",
        "![Screenshot 2024-10-30 at 11.29.25 AM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6732d0876d3daa19fef993ba_Screenshot%202024-11-11%20at%207.50.21%E2%80%AFPM.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYgzCTWg3Nw5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}